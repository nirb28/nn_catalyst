{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Analysis for Better RÂ² Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m descriptors_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     33\u001b[0m targets_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompiled_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 35\u001b[0m descriptors_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mresolve_path_gdrive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescriptors_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     36\u001b[0m targets_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(resolve_path_gdrive(targets_path))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Select target column (dipole_n)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mresolve_path_gdrive\u001b[1;34m(relativePath)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m relativePath\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_project_root\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_project_root() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m relativePath\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xlsxwriter\n",
    "\n",
    "if 'xlsxwriter' not in sys.modules:\n",
    "    !pip install xlsxwriter\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "# Load the data\n",
    "descriptors_path = 'descriptors.csv'\n",
    "targets_path = 'compiled_data.csv'\n",
    "\n",
    "descriptors_df = pd.read_csv(resolve_path_gdrive(descriptors_path))\n",
    "targets_df = pd.read_csv(resolve_path_gdrive(targets_path))\n",
    "\n",
    "# Select target column (dipole_n)\n",
    "selected_cols = 5\n",
    "targets_df = targets_df.iloc[:, [0, selected_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only numeric columns\n",
    "descriptors_numeric = descriptors_df.select_dtypes(include=['number'])\n",
    "targets_numeric = targets_df.select_dtypes(include=['number'])\n",
    "\n",
    "# Merge the numeric dataframes on the common label column\n",
    "numeric_data = pd.merge(descriptors_numeric, targets_numeric, left_on='Label', right_on='mol_num')\n",
    "numeric_data = numeric_data.drop(columns=['Label', 'mol_num'])\n",
    "\n",
    "# Separate features and targets\n",
    "X = numeric_data.iloc[:, :-1]\n",
    "y = numeric_data.iloc[:, -1]\n",
    "\n",
    "# Apply variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_high_variance = selector.fit_transform(X)\n",
    "\n",
    "# Apply feature selection\n",
    "k_best = 500  # You can adjust this value\n",
    "selector = SelectKBest(score_func=f_regression, k=k_best)\n",
    "X_selected = selector.fit_transform(X_high_variance, y)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = X_selected\n",
    "y = y.values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "scaler_y = StandardScaler().fit(y_train)\n",
    "\n",
    "X_train = scaler_X.transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRegressionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, dropout_rate=0.3):\n",
    "        super(ImprovedRegressionNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate():\n",
    "    model = ImprovedRegressionNetwork(X_train.shape[1], 1024, 512, 256, 1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    num_epochs = 300\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets.squeeze())\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_and_evaluate()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(targets.squeeze().cpu().numpy())\n",
    "\n",
    "predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n",
    "\n",
    "r2 = r2_score(actuals, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "print(f'RÂ² Score: {r2:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "\n",
    "# Create Excel file with results\n",
    "output_path = 'improved_model_predictions.xlsx'\n",
    "writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
    "workbook = writer.book\n",
    "worksheet = workbook.add_worksheet('Results')\n",
    "\n",
    "results_df = pd.DataFrame({'Actual': actuals, 'Predicted': predictions})\n",
    "results_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "\n",
    "chart = workbook.add_chart({'type': 'scatter'})\n",
    "chart.add_series({\n",
    "    'name': 'Predictions vs Actuals',\n",
    "    'categories': ['Results', 1, 0, len(actuals), 0],\n",
    "    'values': ['Results', 1, 1, len(predictions), 1],\n",
    "    'marker': {'type': 'circle', 'size': 7},\n",
    "})\n",
    "\n",
    "chart.set_title({'name': 'Predictions vs Actuals'})\n",
    "chart.set_x_axis({'name': 'Actual Values'})\n",
    "chart.set_y_axis({'name': 'Predicted Values'})\n",
    "worksheet.insert_chart('E2', chart)\n",
    "\n",
    "worksheet.write('A1', 'Actual')\n",
    "worksheet.write('B1', 'Predicted')\n",
    "worksheet.write('D1', 'Metrics')\n",
    "worksheet.write('D2', 'RÂ²')\n",
    "worksheet.write('D3', 'RMSE')\n",
    "worksheet.write('D4', 'MAE')\n",
    "worksheet.write('E2', r2)\n",
    "worksheet.write('E3', rmse)\n",
    "worksheet.write('E4', mae)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(f'Results saved to {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
