{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirb28/nn_catalyst/blob/main/src/model_new_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def resolve_path_gdrive(relativePath):\n",
        "    if os.path.exists('/content/drive'):\n",
        "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
        "    else:\n",
        "        from utils import get_project_root\n",
        "        return get_project_root() + \"/\" + relativePath\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "metadata": {
        "id": "hoOmBk4-7xn_",
        "outputId": "e182739e-6017-484b-ae3e-8761b4a62d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-09-25T18:20:08.165092Z",
          "start_time": "2024-09-25T18:20:07.468928Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n",
        "import xlsxwriter"
      ],
      "metadata": {
        "id": "bXsAGjvscr-3",
        "outputId": "d7ff54f9-bc8d-4e69-d650-411ced3fb3d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-09-25T18:20:26.588654Z",
          "start_time": "2024-09-25T18:20:20.292835Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/159.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npSP_nXh7CNA",
        "ExecuteTime": {
          "end_time": "2024-09-25T18:20:34.053899Z",
          "start_time": "2024-09-25T18:20:26.592650Z"
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import xlsxwriter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Load the data\n",
        "descriptors_path = 'descriptors.csv'\n",
        "targets_path = 'compiled_data.csv'"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "#descriptors_df = pd.read_csv(resolve_path_gdrive(descriptors_path))\n",
        "#targets_df = pd.read_csv(resolve_path_gdrive(targets_path))\n",
        "numeric_data = np.loadtxt(resolve_path_gdrive('src/pl/merged_data_last29.csv'), delimiter=',', skiprows=1, max_rows=None, dtype=float)"
      ],
      "metadata": {
        "id": "wKrIMr8_7Gnk",
        "ExecuteTime": {
          "end_time": "2024-09-25T18:20:34.132819Z",
          "start_time": "2024-09-25T18:20:34.058008Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "dyEEJUpFv5Xt",
        "ExecuteTime": {
          "end_time": "2024-09-25T18:20:44.105042Z",
          "start_time": "2024-09-25T18:20:44.076554Z"
        }
      },
      "cell_type": "code",
      "source": [
        "number_of_target_cols = 29\n",
        "# Separate features and targets\n",
        "X = numeric_data[:, :-number_of_target_cols]  # Assuming the last 30 columns are targets\n",
        "y = numeric_data[:, -number_of_target_cols:]\n",
        "\n",
        "# Use only 1 column\n",
        "#y=y[:,:1]\n",
        "\n",
        "# Apply variance threshold\n",
        "selector = VarianceThreshold()\n",
        "X_high_variance = selector.fit_transform(X)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = X_high_variance\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler_X = StandardScaler().fit(X_train)\n",
        "scaler_y = StandardScaler().fit(y_train)\n",
        "\n",
        "X_train = scaler_X.transform(X_train)\n",
        "X_val = scaler_X.transform(X_val)\n",
        "X_test = scaler_X.transform(X_test)\n",
        "\n",
        "y_train = scaler_y.transform(y_train)\n",
        "y_val = scaler_y.transform(y_val)\n",
        "y_test = scaler_y.transform(y_test)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n"
      ],
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for batch processing\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Define the individual model class\n",
        "class SingleTargetNet(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(SingleTargetNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, 1)\n",
        "        self.fc_skip = nn.Linear(1024, 512)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
        "        x1 = self.dropout(x1)\n",
        "\n",
        "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
        "        x2 = self.dropout(x2)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 += self.fc_skip(x1)\n",
        "\n",
        "        x3 = self.fc3(x2)\n",
        "        return x3\n",
        "\n",
        "# Function to train and evaluate individual models\n",
        "def train_and_evaluate(target_index):\n",
        "    model = SingleTargetNet(X_train.shape[1])\n",
        "    model.to(device=device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    patience_counter = 0\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets[:, target_index])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Target {target_index} - Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs).squeeze()\n",
        "                loss = criterion(outputs, targets[:, target_index])\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f'Target {target_index} - Validation Loss: {val_loss}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'best_model_target_{target_index}.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= 15:\n",
        "                print(f'Target {target_index} - Early stopping triggered')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(f'best_model_target_{target_index}.pth'))\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets[:, target_index])\n",
        "            test_loss += loss.item()\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f'Target {target_index} - Test Loss: {test_loss}')\n",
        "\n",
        "    return model, test_loss\n"
      ],
      "metadata": {
        "id": "o3VsGc6ilU_w",
        "ExecuteTime": {
          "end_time": "2024-09-25T18:20:46.634109Z",
          "start_time": "2024-09-25T18:20:46.607154Z"
        }
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# Train and evaluate individual models for each target\n",
        "test_losses = []\n",
        "models = []\n",
        "for target_index in range(y_train.shape[1]):\n",
        "    model, test_loss = train_and_evaluate(target_index)\n",
        "    models.append(model)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "# Print average test loss\n",
        "avg_test_loss = np.mean(test_losses)\n",
        "print(f'Average Test Loss for Individual Models: {avg_test_loss}')\n",
        "\n",
        "# Initialize Excel writer\n",
        "output_path = 'new_stn_model_predictions_with_plots.xlsx'\n",
        "writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
        "workbook = writer.book\n",
        "\n",
        "# Create necessary Excel sheets\n",
        "train_sheet = workbook.add_worksheet('Train')\n",
        "val_sheet = workbook.add_worksheet('Validation')\n",
        "test_sheet = workbook.add_worksheet('Test')\n",
        "\n",
        "# Prepare DataFrames for train, validation, and test predictions\n",
        "train_df = pd.DataFrame()\n",
        "val_df = pd.DataFrame()\n",
        "test_df = pd.DataFrame()\n",
        "\n",
        "r2_scores, rmse_scores, mae_scores = [], [], []\n",
        "\n",
        "def create_excel_chart(sheet_name, target_index, worksheet, df, start_row, start_col):\n",
        "    chart = workbook.add_chart({'type': 'scatter'})\n",
        "\n",
        "    observed_col = f'Observed_{target_index}'\n",
        "    predicted_col = f'Predicted_{target_index}'\n",
        "\n",
        "    chart.add_series({\n",
        "        'name': f'Target {target_index}',\n",
        "        'categories': [sheet_name, start_row+1, df.columns.get_loc(observed_col), start_row+df.shape[0], df.columns.get_loc(observed_col)],\n",
        "        'values': [sheet_name, start_row+1, df.columns.get_loc(predicted_col), start_row+df.shape[0], df.columns.get_loc(predicted_col)],\n",
        "        'marker': {'type': 'circle', 'size': 5},\n",
        "        'trendline': {\n",
        "            'type': 'linear',\n",
        "            'display_equation': True,\n",
        "            'display_r_squared': True,\n",
        "        }\n",
        "    })\n",
        "    chart.set_title({'name': f'Parity Plot for Target {target_index}'})\n",
        "    chart.set_x_axis({'name': 'Observed'})\n",
        "    chart.set_y_axis({'name': 'Predicted'})\n",
        "    chart.set_legend({'none': True})\n",
        "\n",
        "    # Make axes square with the same unit ranges on x and y axis\n",
        "    min_val = min(df[observed_col].min(), df[predicted_col].min())\n",
        "    max_val = max(df[observed_col].max(), df[predicted_col].max())\n",
        "    chart.set_x_axis({'min': min_val, 'max': max_val})\n",
        "    chart.set_y_axis({'min': min_val, 'max': max_val})\n",
        "\n",
        "    worksheet.insert_chart(start_row + df.shape[0] + 2, start_col, chart)\n",
        "\n",
        "    # Calculate metrics\n",
        "    observed = df[observed_col]\n",
        "    predicted = df[predicted_col]\n",
        "    r2 = r2_score(observed, predicted)\n",
        "    if math.isnan(r2):\n",
        "        r2 = 0.0\n",
        "    rmse = mean_squared_error(observed, predicted, squared=False)\n",
        "    mae = mean_absolute_error(observed, predicted)\n",
        "\n",
        "    # Write metrics to Excel\n",
        "    metrics_start_row = start_row + df.shape[0] + 22\n",
        "    worksheet.write(metrics_start_row, start_col, f'Target {target_index}')\n",
        "    worksheet.write(metrics_start_row + 1, start_col + 1, 'R²')\n",
        "    worksheet.write(metrics_start_row + 1, start_col + 2, r2)\n",
        "    worksheet.write(metrics_start_row + 2, start_col + 1, 'RMSE')\n",
        "    worksheet.write(metrics_start_row + 2, start_col + 2, rmse)\n",
        "    worksheet.write(metrics_start_row + 3, start_col + 1, 'MAE')\n",
        "    worksheet.write(metrics_start_row + 3, start_col + 2, mae)\n",
        "\n",
        "    return r2, rmse, mae\n",
        "\n",
        "for target_index in range(y_train.shape[1]):\n",
        "    model = models[target_index]\n",
        "\n",
        "    # Make predictions on the train, validation, and test sets\n",
        "    model.cpu().eval()\n",
        "    with torch.no_grad():\n",
        "        y_train_pred = model(X_train_tensor.cpu()).numpy()\n",
        "        y_val_pred = model(X_val_tensor.cpu()).numpy()\n",
        "        y_test_pred = model(X_test_tensor.cpu()).numpy()\n",
        "\n",
        "    # Inverse transform the predictions and targets to their original scale\n",
        "    y_train_pred_orig = scaler_y.inverse_transform(np.concatenate([np.zeros((y_train_pred.shape[0], target_index)), y_train_pred, np.zeros((y_train_pred.shape[0], y_train.shape[1] - target_index - 1))], axis=1))[:, target_index]\n",
        "    y_val_pred_orig = scaler_y.inverse_transform(np.concatenate([np.zeros((y_val_pred.shape[0], target_index)), y_val_pred, np.zeros((y_val_pred.shape[0], y_val.shape[1] - target_index - 1))], axis=1))[:, target_index]\n",
        "    y_test_pred_orig = scaler_y.inverse_transform(np.concatenate([np.zeros((y_test_pred.shape[0], target_index)), y_test_pred, np.zeros((y_test_pred.shape[0], y_test.shape[1] - target_index - 1))], axis=1))[:, target_index]\n",
        "\n",
        "    y_train_orig = scaler_y.inverse_transform(y_train)[:, target_index]\n",
        "    y_val_orig = scaler_y.inverse_transform(y_val)[:, target_index]\n",
        "    y_test_orig = scaler_y.inverse_transform(y_test)[:, target_index]\n",
        "\n",
        "    # Create dataframes for the predictions and actual values\n",
        "    train_df[f'Observed_{target_index}'] = y_train_orig\n",
        "    train_df[f'Predicted_{target_index}'] = y_train_pred_orig\n",
        "\n",
        "    val_df[f'Observed_{target_index}'] = y_val_orig\n",
        "    val_df[f'Predicted_{target_index}'] = y_val_pred_orig\n",
        "\n",
        "    test_df[f'Observed_{target_index}'] = y_test_orig\n",
        "    test_df[f'Predicted_{target_index}'] = y_test_pred_orig\n",
        "\n",
        "    # Create and insert parity plots for train, validation, and test sets\n",
        "    r2, rmse, mae = create_excel_chart('Train', target_index, train_sheet, train_df, start_row=0, start_col=target_index*9)\n",
        "    r2_scores.append(r2)\n",
        "    rmse_scores.append(rmse)\n",
        "    mae_scores.append(mae)\n",
        "    create_excel_chart('Validation', target_index, val_sheet, val_df, start_row=0, start_col=target_index*9)\n",
        "    create_excel_chart('Test', target_index, test_sheet, test_df, start_row=0, start_col=target_index*9)\n",
        "\n",
        "# Write dataframes to Excel sheets\n",
        "train_df.to_excel(writer, sheet_name='Train', index=False)\n",
        "val_df.to_excel(writer, sheet_name='Validation', index=False)\n",
        "test_df.to_excel(writer, sheet_name='Test', index=False)\n",
        "\n",
        "# Save and close the Excel file\n",
        "writer.close()\n",
        "\n",
        "# Calculate and print the average R², RMSE, and MAE for the validation set\n",
        "avg_r2 = np.mean(r2_scores)\n",
        "avg_rmse = np.mean(rmse_scores)\n",
        "avg_mae = np.mean(mae_scores)\n",
        "\n",
        "print(f\"Average R² for Validation Set: {avg_r2}\")\n",
        "print(f\"Average RMSE for Validation Set: {avg_rmse}\")\n",
        "print(f\"Average MAE for Validation Set: {avg_mae}\")\n",
        "\n",
        "print(f\"Predictions and plots written to {output_path}\")\n"
      ],
      "metadata": {
        "id": "ttKew0QJIKlZ",
        "outputId": "29dac87c-922e-40b0-dbd3-1b8bcc13e0aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-09-25T18:33:28.538926Z",
          "start_time": "2024-09-25T18:33:27.510875Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target 0 - Epoch 1/5, Loss: 0.42820434093975074\n",
            "Target 0 - Validation Loss: 0.1995412760028025\n",
            "Target 0 - Epoch 2/5, Loss: 0.24263000171404422\n",
            "Target 0 - Validation Loss: 0.1700401059919741\n",
            "Target 0 - Epoch 3/5, Loss: 0.23964180420266418\n",
            "Target 0 - Validation Loss: 0.14529863899437392\n",
            "Target 0 - Epoch 4/5, Loss: 0.21904857667935332\n",
            "Target 0 - Validation Loss: 0.1688690684372332\n",
            "Target 0 - Epoch 5/5, Loss: 0.21586658394463906\n",
            "Target 0 - Validation Loss: 0.14804663763540546\n",
            "Target 0 - Test Loss: 0.15373260355213794\n",
            "Average Test Loss for Individual Models: 0.15373260355213794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-bee119f0daaa>:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f'best_model_target_{target_index}.pth'))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average R² for Validation Set: 0.8535797802253327\n",
            "Average RMSE for Validation Set: 0.009236143386642788\n",
            "Average MAE for Validation Set: 0.006990942119437592\n",
            "Predictions and plots written to new_stn_model_predictions_with_plots.xlsx\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3hDknhV6ebj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}