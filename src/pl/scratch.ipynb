{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirb28/nn_catalyst/blob/main/src/pl/scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "DEBUG = False\n",
        "\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Colab!\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    !pip install pytorch_lightning\n",
        "    !pip install torchmetrics\n",
        "else:\n",
        "    print(\"Not running in Colab.\")"
      ],
      "metadata": {
        "id": "AKO5oAFESNmd",
        "ExecuteTime": {
          "end_time": "2024-10-12T22:45:10.174589Z",
          "start_time": "2024-10-12T22:45:10.147728Z"
        }
      },
      "id": "AKO5oAFESNmd",
      "outputs": [],
      "execution_count": 18
    },
    {
      "metadata": {
        "id": "88eb903e9b5a6f6f",
        "ExecuteTime": {
          "end_time": "2024-10-12T23:14:24.282268Z",
          "start_time": "2024-10-12T23:14:24.270271Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Training hyperparameters\n",
        "INPUT_SIZE = 1479\n",
        "NUM_TARGETS = 1\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "# Dataset\n",
        "DATA_DIR = \"dataset/\"\n",
        "NUM_WORKERS = 15\n",
        "\n",
        "# Compute related\n",
        "ACCELERATOR = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICES = [0]\n",
        "PRECISION = 32"
      ],
      "id": "88eb903e9b5a6f6f",
      "outputs": [],
      "execution_count": 19
    },
    {
      "metadata": {
        "id": "25fba0fd40809552",
        "outputId": "fcfbdb85-260d-4364-c493-d22e2e04d264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-10-12T23:14:26.495895Z",
          "start_time": "2024-10-12T23:14:26.454728Z"
        }
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import random_split\n",
        "import pytorch_lightning as pl\n",
        "import torch, math, os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pl.seed_everything(1234)\n",
        "\n",
        "def resolve_path_gdrive(relativePath):\n",
        "    if os.path.exists('/content/drive'):\n",
        "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
        "    else:\n",
        "        from utils import get_project_root\n",
        "        return get_project_root() + \"/\" + relativePath\n"
      ],
      "id": "25fba0fd40809552",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 1234\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-12T23:14:25.726729Z",
          "start_time": "2024-10-12T23:14:25.667831Z"
        },
        "id": "31982c159904e778"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "datafile='src/pl/merged_data_last29_reordered_byR2.csv'\n",
        "xy_orig = np.loadtxt(resolve_path_gdrive(datafile), delimiter=',', skiprows=1, dtype=float, max_rows=None)"
      ],
      "id": "31982c159904e778",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CatalystDataset(Dataset):\n",
        "    def __init__(self, target_num, datafile='src/pl/merged_data_last29_reordered_byR2.csv',\n",
        "                 useTargetsAsFeatures=False):\n",
        "        # Initialize data, download, etc.\n",
        "        #xy = np.loadtxt(resolve_path_gdrive(datafile), delimiter=',', skiprows=1, dtype=float, max_rows=None)\n",
        "        xy = xy_orig.copy()\n",
        "        #xy = StandardScaler().fit_transform(xy)\n",
        "        self.n_samples = xy.shape[0]\n",
        "        # here the first column is the class label, the rest are the features\n",
        "        total_targets = 29\n",
        "        target_col_start = -(total_targets - target_num + 1)\n",
        "        target_col_end = target_col_start + 1\n",
        "        if useTargetsAsFeatures:\n",
        "            self.x_data = torch.from_numpy(xy[:,:target_col_start]).float()  # size [n_samples, n_features]\n",
        "        else:\n",
        "            self.x_data = torch.from_numpy(xy[:,:-total_targets]).float()  # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(xy[:,target_col_start:target_col_end]).float()  # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.x_data[index], self.y_data[index]\n",
        "        return sample\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "class CatalystDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size, num_workers, target_num):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.entire_dataset = CatalystDataset(target_num=target_num,\n",
        "                                              useTargetsAsFeatures=True)\n",
        "        self.target_num = target_num\n",
        "\n",
        "    def prepare_data(self):\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        train_set_size = int(len(self.entire_dataset) * 0.8)\n",
        "        test_set_size = int(len(self.entire_dataset) * 0.1)\n",
        "        valid_set_size = len(self.entire_dataset) - train_set_size - test_set_size\n",
        "        print(f\"Train set size: {train_set_size}, Test set size: {test_set_size}, Valid set size: {valid_set_size}\")\n",
        "        self.train_ds, self.val_ds, self.test_ds = random_split(\n",
        "            self.entire_dataset, [train_set_size, valid_set_size, test_set_size])\n",
        "        return\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "dm1 = CatalystDataModule(\n",
        "        data_dir=DATA_DIR,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        target_num=3\n",
        "    )\n",
        "dm1.prepare_data()\n",
        "dm1.setup()"
      ],
      "metadata": {
        "id": "_v-vj-X2BlyQ"
      },
      "id": "_v-vj-X2BlyQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-12T23:14:27.485106Z",
          "start_time": "2024-10-12T23:14:27.472061Z"
        },
        "id": "bcecf394022d918"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "for x,y in dm1.test_dataloader():\n",
        "    #print(x)\n",
        "    print(y)\n",
        "'''"
      ],
      "id": "bcecf394022d918",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2c407a8a7a5bc7db",
        "ExecuteTime": {
          "end_time": "2024-10-12T23:14:28.397715Z",
          "start_time": "2024-10-12T23:14:28.361680Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "from torchmetrics.regression import R2Score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaseModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.r2 = torchmetrics.R2Score()\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log_dict(\n",
        "            {\n",
        "                \"train_loss\": loss,\n",
        "            },\n",
        "            on_step=True,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "        )\n",
        "        accuracy = self.r2(scores, y)\n",
        "        self.log(\"train_acc\", accuracy, prog_bar=True)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.validation_step_outputs.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
        "        self.log(\"validation_epoch_average\", epoch_average)\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def _common_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        scores = self.forward(x)\n",
        "        loss = self.loss_fn(scores, y)\n",
        "        if DEBUG == True:\n",
        "            print(f\"loss: {loss}, len: {len(y)}\")\n",
        "        return loss, scores, y\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        scores = self.forward(x)\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        return preds\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0.000000001, threshold=0.001)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
        "\n",
        "    def on_load_checkpoint(self, checkpoint: dict) -> None:\n",
        "        state_dict = checkpoint[\"state_dict\"]\n",
        "        model_state_dict = self.state_dict()\n",
        "        is_changed = False\n",
        "        for k in state_dict:\n",
        "            if k in model_state_dict:\n",
        "                if state_dict[k].shape != model_state_dict[k].shape:\n",
        "                    print(f\"Skip loading parameter: {k}, \"\n",
        "                                f\"required shape: {model_state_dict[k].shape}, \"\n",
        "                                f\"loaded shape: {state_dict[k].shape}\")\n",
        "                    state_dict[k] = model_state_dict[k]\n",
        "                    is_changed = True\n",
        "            else:\n",
        "                is_changed = True\n",
        "\n",
        "        if is_changed:\n",
        "            checkpoint.pop(\"optimizer_states\", None)\n",
        "\n",
        "class SingleTargetNet(BaseModel):\n",
        "\n",
        "    def __init__(self, input_size=INPUT_SIZE, learning_rate=0.001, dropout_rate=0.5, target=1):\n",
        "        super(SingleTargetNet, self).__init__()\n",
        "        self.lr = learning_rate\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, 1)\n",
        "        self.fc_skip = nn.Linear(1024, 512)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
        "        x1 = self.dropout(x1)\n",
        "\n",
        "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
        "        x2 = self.dropout(x2)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 += self.fc_skip(x1)\n",
        "\n",
        "        x3 = self.fc3(x2)\n",
        "        return x3\n",
        ""
      ],
      "id": "2c407a8a7a5bc7db",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "451f92a00a4137fa",
        "ExecuteTime": {
          "end_time": "2024-10-12T23:14:41.289624Z",
          "start_time": "2024-10-12T23:14:30.519350Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "#from model import NN\n",
        "#from dataset import CatalystDataModule\n",
        "#from config import *\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "torch.set_float32_matmul_precision(\"medium\") # to make lightning happy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # do everything in a loop for all the targets\n",
        "    for target in range(1, 30):\n",
        "        print(f\"Target: {target}\")\n",
        "        dm = CatalystDataModule(\n",
        "            data_dir=DATA_DIR,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            num_workers=NUM_WORKERS,\n",
        "            target_num=target\n",
        "        )\n",
        "        dm.prepare_data()\n",
        "        dm.setup()\n",
        "        print(f\"Train set size: {len(dm.train_ds),dm.train_ds.dataset.x_data.shape[1]}\")\n",
        "        print(f\"Test set size: {len(dm.test_ds)}, Valid set size: {len(dm.val_ds)}\")\n",
        "\n",
        "        model = SingleTargetNet (\n",
        "            input_size=dm.entire_dataset.x_data.shape[1],\n",
        "            learning_rate=LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "        # DEFAULTS used by the Trainer\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            dirpath=resolve_path_gdrive(f'/checkpoints/stn_1/{dm.target_num}'),\n",
        "            filename='{epoch:02d}-{val_loss:.2f}',\n",
        "            save_top_k=3,\n",
        "            #save_best_only=True,\n",
        "            verbose=True,\n",
        "            monitor='val_loss',\n",
        "            mode='min'\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            accelerator=ACCELERATOR,\n",
        "            devices=1,\n",
        "            min_epochs=1,\n",
        "            max_epochs=NUM_EPOCHS,\n",
        "            precision=PRECISION,\n",
        "            fast_dev_run=False,\n",
        "            enable_checkpointing=True,\n",
        "            callbacks=[checkpoint_callback,\n",
        "                    EarlyStopping(monitor=\"train_loss\", patience=10, verbose=True, mode=\"min\")]\n",
        "        )\n",
        "        trainer.fit(model, dm)\n",
        "        trainer.validate(model, dm)\n",
        "        trainer.test(model, dm)"
      ],
      "id": "451f92a00a4137fa",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#.best_model_path\n",
        "os.rename(checkpoint_callback.best_model_path, checkpoint_callback.dirpath+\"/best.ckpt\")"
      ],
      "metadata": {
        "id": "lFca1ZjiCwt5"
      },
      "id": "lFca1ZjiCwt5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the best checkpoint\n",
        "model = SingleTargetNet.load_from_checkpoint(checkpoint_callback.dirpath+\"/best.ckpt\")\n",
        "model.eval()\n",
        "model.cpu()"
      ],
      "metadata": {
        "id": "tE36hbRA3tRw"
      },
      "id": "tE36hbRA3tRw",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f9ceeceac39b7c6d"
      },
      "cell_type": "code",
      "source": [
        "from torchmetrics import R2Score\n",
        "from torchmetrics import MeanSquaredError\n",
        "\n",
        "def r2scoreAndMSE(model, dataloader):\n",
        "    r2_score_metric = R2Score()\n",
        "    mse = MeanSquaredError()\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        predictions = model(data)\n",
        "        #import pdb; pdb.set_trace()\n",
        "        r2_score_metric.update(predictions, target)\n",
        "        mse.update(predictions, target)\n",
        "    return r2_score_metric.compute(), mse.compute()"
      ],
      "id": "f9ceeceac39b7c6d",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2scoreAndMSE(model, dm1.train_dataloader()))\n",
        "print(r2scoreAndMSE(model, dm1.val_dataloader()))\n",
        "print(r2scoreAndMSE(model, dm1.test_dataloader()))"
      ],
      "metadata": {
        "id": "2rY85I_NJ8K8"
      },
      "id": "2rY85I_NJ8K8",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Try inference with checkpoint\n",
        "checkpoint_path = checkpoint_callback.dirpath+\"/best.ckpt\"\n",
        "model1 = SingleTargetNet.load_from_checkpoint(checkpoint_path)\n",
        "model1.eval()\n",
        "model1.cpu()\n",
        "r2scoreAndMSE(model1, dm1.val_dataloader())"
      ],
      "metadata": {
        "id": "CCqQmYyWlETp"
      },
      "id": "CCqQmYyWlETp",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "matrix = np.array([[1, 2, 3, 4],\n",
        "                   [5, 6, 7, 8],\n",
        "                   [9, 10, 11, 12]])"
      ],
      "metadata": {
        "id": "u1IP95JW5JPT"
      },
      "id": "u1IP95JW5JPT",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "m = np.delete(matrix, [-1], axis=1)\n",
        "m"
      ],
      "metadata": {
        "id": "0atVEhkXGKHN"
      },
      "id": "0atVEhkXGKHN",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEA9-WzTGPGL"
      },
      "id": "MEA9-WzTGPGL",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}