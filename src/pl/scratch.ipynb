{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nirb28/nn_catalyst/blob/main/src/pl/scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "99af9e205d508ca4"
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Colab!\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install torchmetrics\n",
    "else:\n",
    "    print(\"Not running in Colab.\")"
   ],
   "metadata": {
    "id": "AKO5oAFESNmd"
   },
   "id": "AKO5oAFESNmd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "febc06fcc3bc3638",
    "ExecuteTime": {
     "end_time": "2024-11-12T18:32:10.335904Z",
     "start_time": "2024-11-12T18:32:10.304200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "DEBUG = False\n",
    "# Training hyperparameters\n",
    "INPUT_SIZE = 1479\n",
    "NUM_TARGETS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 150\n",
    "NUM_WORKERS = 0\n",
    "# Compute related\n",
    "ACCELERATOR = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICES = [0]\n",
    "PRECISION = 32\n",
    "CHECKPOINTS_FOLDER = \"/checkpoints/stn_2_r2\" #tmp_r1\""
   ],
   "id": "febc06fcc3bc3638",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "id": "25fba0fd40809552",
    "outputId": "8d2dccb9-18d3-4015-f7ec-d3dc9f60ad7c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "import torch, math, os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "seed = 1234\n",
    "pl.seed_everything(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision(\"medium\") # to make lightning happy\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")"
   ],
   "id": "25fba0fd40809552",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "88eb903e9b5a6f6f",
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir f\"/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/checkpoints/{CHECKPOINTS_FOLDER}/lightning_logs\""
   ],
   "id": "88eb903e9b5a6f6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "31982c159904e778"
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "datafile='src/pl/merged_data_last29_reordered_byR2.csv'\n",
    "max_rows=None\n",
    "xy_orig = np.loadtxt(resolve_path_gdrive(datafile), delimiter=',', skiprows=1, dtype=float, max_rows=max_rows)"
   ],
   "id": "31982c159904e778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2c407a8a7a5bc7db"
   },
   "cell_type": "code",
   "source": [
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r2 = torchmetrics.R2Score()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        accuracy = self.r2(scores, y)\n",
    "        self.log(\"train_acc\", accuracy, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        if DEBUG == True:\n",
    "            print(f\"loss: {loss}, len: {len(y)}\")\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0.000000001, threshold=0.001)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "class SingleTargetNet(BaseModel):\n",
    "\n",
    "    def __init__(self, input_size=INPUT_SIZE, learning_rate=0.001, dropout_rate=0.5, target=1):\n",
    "        super(SingleTargetNet, self).__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc_skip = nn.Linear(1024, 512)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 += self.fc_skip(x1)\n",
    "\n",
    "        x3 = self.fc3(x2)\n",
    "        return x3\n"
   ],
   "id": "2c407a8a7a5bc7db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt: write a function that takes in a numpy array and splits it into train, test and validation. it then scales all the data including the target columns. finally create a dataset and dataloader for all the 3 and wrap it into a datamodule\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class CatalystDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Split data into train, validation, and test sets\n",
    "        X = self.data[:, :-1]\n",
    "        y = self.data[:, -1]\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Scale data using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "        y_train = scaler.fit_transform(y_train)\n",
    "        y_val = scaler.transform(y_val)\n",
    "        y_test = scaler.transform(y_test)\n",
    "\n",
    "        # Create numpy arrays for the data\n",
    "        self.train_data = np.concatenate((X_train, y_train), axis=1)\n",
    "        self.val_data = np.concatenate((X_val, y_val), axis=1)\n",
    "        self.test_data = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Create datasets\n",
    "        self.train_dataset = CatalystDataset(self.train_data)\n",
    "        self.val_dataset = CatalystDataset(self.val_data)\n",
    "        self.test_dataset = CatalystDataset(self.test_data)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "class CatalystDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.x = torch.tensor(self.data[:, :-1], dtype=torch.float32)\n",
    "        #self.y = torch.tensor(self.data[:, -1], dtype=torch.float32)\n",
    "        self.y = torch.unsqueeze(\n",
    "            torch.tensor(self.data[:, -1], dtype=torch.float32), 1).float()  # size [n_samples, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ],
   "metadata": {
    "id": "KsMrlWgAD9mj"
   },
   "id": "KsMrlWgAD9mj",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "104ba4b8ac4936f0"
   },
   "cell_type": "code",
   "source": [
    "from torch import nn, optim\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "\n",
    "def prepare_data_module(xy):\n",
    "    dm = CatalystDataModule(data=xy)\n",
    "    dm.prepare_data()\n",
    "    dm.setup()\n",
    "    return dm\n",
    "\n",
    "def prepare_trainer(target, num_epochs=NUM_EPOCHS):\n",
    "    tensorboard = TensorBoardLogger(resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/lightning_logs'), name=f\"{target}\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/{target}'),\n",
    "        filename='{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=ACCELERATOR,\n",
    "        devices=1,\n",
    "        min_epochs=1,\n",
    "        max_epochs=num_epochs,\n",
    "        precision=PRECISION,\n",
    "        fast_dev_run=True,\n",
    "        enable_checkpointing=True,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=20,\n",
    "        logger=tensorboard,\n",
    "        callbacks=[checkpoint_callback, lr_monitor, RichProgressBar(),\n",
    "                EarlyStopping(monitor=\"train_loss\", patience=10, verbose=True, mode=\"min\")]\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def prepare_model(input_size):\n",
    "    model = SingleTargetNet (\n",
    "        input_size=input_size,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def iterate_all_targets(xy_data, total_targets, target_start_range=1, target_stop_range=None):\n",
    "    total_cols = xy_data.shape[1]\n",
    "    if target_stop_range == None: \n",
    "        target_stop_range = total_targets\n",
    "    for target_num in range(target_start_range, target_stop_range+1):\n",
    "        target_col_start = total_cols - (total_targets - target_num)\n",
    "        print(f'Target {target_num}, target_col {target_col_start}')\n",
    "        xy_data = torch.from_numpy(xy_orig[:,:target_col_start]).float()  # size [n_samples, n_features]\n",
    "        dm = prepare_data_module(xy_data)\n",
    "        model = prepare_model(input_size=dm.train_dataset.x.shape[1])\n",
    "        trainer = prepare_trainer(target=target_num, num_epochs=NUM_EPOCHS)\n",
    "        trainer.fit(model, dm)\n",
    "        trainer.validate(model, dm)\n",
    "        trainer.test(model, dm)\n"
   ],
   "id": "104ba4b8ac4936f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ******** TRAINING CELL *********\n",
    "TRAINING = False\n",
    "if TRAINING:\n",
    "    iterate_all_targets(xy_orig, total_targets=29, target_start_range=1, target_stop_range=5)"
   ],
   "id": "da52331a3a11e24d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f9ceeceac39b7c6d",
    "ExecuteTime": {
     "end_time": "2024-11-12T18:32:40.568597Z",
     "start_time": "2024-11-12T18:32:40.536785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchmetrics import R2Score\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "def r2scoreAndMSE(model, dataloader):\n",
    "    r2_score_metric = R2Score()\n",
    "    mse = MeanSquaredError()\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        predictions = model(data)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        r2_score_metric.update(predictions, target)\n",
    "        mse.update(predictions, target)\n",
    "    return r2_score_metric.compute().detach().item(), mse.compute().detach().item()"
   ],
   "id": "f9ceeceac39b7c6d",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:43:25.733676Z",
     "start_time": "2024-11-12T18:43:11.276774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "\n",
    "def measure_all_targets(xy_data, total_targets, target_start_range=1, target_stop_range=None):\n",
    "    total_cols = xy_data.shape[1]\n",
    "    if target_stop_range == None: \n",
    "        target_stop_range = total_targets\n",
    "    for target_num in range(target_start_range, target_stop_range+1):\n",
    "        target_col_start = total_cols - (total_targets - target_num)\n",
    "        print(f'Target {target_num}, target_col {target_col_start}')\n",
    "        xy_data = torch.from_numpy(xy_orig[:,:target_col_start]).float()  # size [n_samples, n_features]\n",
    "        eval_model(xy_data, target_num)\n",
    "        \n",
    "def eval_model(xy_data, target_num):\n",
    "        dm = prepare_data_module(xy_data)\n",
    "        if DEBUG == True:\n",
    "            print(f\"Train set size: {len(dm.train_ds),dm.train_ds.dataset.x.shape[1]}\")\n",
    "            print(f\"Test set size: {len(dm.test_ds)}, Valid set size: {len(dm.val_ds)}\")\n",
    "\n",
    "        checkpoint_path=resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/{target_num}')\n",
    "        pathlist = Path(checkpoint_path).glob('**/*.ckpt')\n",
    "        for path in pathlist:\n",
    "            # because path is object not string\n",
    "            model = SingleTargetNet.load_from_checkpoint(str(path))\n",
    "            model.eval()\n",
    "            model.cpu()\n",
    "            # add row to resultsDF\n",
    "            train_r2, train_mse = r2scoreAndMSE(model, dm.train_dataloader())\n",
    "            val_r2, val_mse = r2scoreAndMSE(model, dm.val_dataloader())\n",
    "            test_r2, test_mse = r2scoreAndMSE(model, dm.test_dataloader())\n",
    "\n",
    "            results.append([target_num, os.path.basename(path), train_r2, train_mse, val_r2, val_mse, test_r2, test_mse])    \n",
    "\n",
    "TESTING = True\n",
    "results = []  \n",
    "if TESTING:\n",
    "    measure_all_targets(xy_orig, total_targets=29,target_start_range=1, target_stop_range=5)"
   ],
   "id": "9dad6061d6b9e738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target 1, target_col 1480\n",
      "Target 2, target_col 1481\n",
      "Target 3, target_col 1482\n",
      "Target 4, target_col 1483\n",
      "Target 5, target_col 1484\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "id": "3af373f6ab2cec1a",
    "outputId": "04ac3e88-cb99-4862-b47b-35cbc05c5355",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "ExecuteTime": {
     "end_time": "2024-11-12T18:43:25.764681Z",
     "start_time": "2024-11-12T18:43:25.738683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "resultsDFcolumns = [\"Target\", \"ModelFile\", \"Train R2\", \"Train MSE\", \"Val R2\", \"Val MSE\", \"Test R2\", \"Test MSE\"]\n",
    "resultsDF = pd.DataFrame(results, columns=resultsDFcolumns)\n",
    "resultsDF"
   ],
   "id": "3af373f6ab2cec1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Target                    ModelFile  Train R2  Train MSE    Val R2  \\\n",
       "0       1  epoch=26-val_loss=0.00.ckpt  0.996414   0.003586  0.996376   \n",
       "1       2  epoch=35-val_loss=0.00.ckpt  0.997682   0.002318  0.997154   \n",
       "2       3  epoch=73-val_loss=0.00.ckpt  0.998498   0.001502  0.997765   \n",
       "3       4  epoch=60-val_loss=0.00.ckpt  0.998913   0.001087  0.998615   \n",
       "4       5  epoch=55-val_loss=0.00.ckpt  0.998662   0.001338  0.998204   \n",
       "\n",
       "    Val MSE   Test R2  Test MSE  \n",
       "0  0.003866  0.996377  0.003434  \n",
       "1  0.003036  0.997258  0.002598  \n",
       "2  0.002384  0.997934  0.001958  \n",
       "3  0.001478  0.998345  0.001568  \n",
       "4  0.001916  0.998139  0.001764  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>ModelFile</th>\n",
       "      <th>Train R2</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Val R2</th>\n",
       "      <th>Val MSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>epoch=26-val_loss=0.00.ckpt</td>\n",
       "      <td>0.996414</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.996376</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.996377</td>\n",
       "      <td>0.003434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>epoch=35-val_loss=0.00.ckpt</td>\n",
       "      <td>0.997682</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.997154</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.997258</td>\n",
       "      <td>0.002598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>epoch=73-val_loss=0.00.ckpt</td>\n",
       "      <td>0.998498</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.997934</td>\n",
       "      <td>0.001958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>epoch=60-val_loss=0.00.ckpt</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.998615</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.998345</td>\n",
       "      <td>0.001568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>epoch=55-val_loss=0.00.ckpt</td>\n",
       "      <td>0.998662</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.998204</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.998139</td>\n",
       "      <td>0.001764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "id": "4398bc53fbf63706"
   },
   "cell_type": "code",
   "source": [
    "resultsDF.to_csv(resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/results.csv'), index=False)"
   ],
   "id": "4398bc53fbf63706",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
