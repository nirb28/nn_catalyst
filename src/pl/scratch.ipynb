{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirb28/nn_catalyst/blob/main/src/pl/scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-24T13:29:42.183123Z",
          "start_time": "2024-09-24T13:29:27.889962Z"
        },
        "id": "25fba0fd40809552"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import random_split\n",
        "import pytorch_lightning as pl\n",
        "import torch, math, os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "def resolve_path_gdrive(relativePath):\n",
        "    if os.path.exists('/content/drive'):\n",
        "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
        "    else:\n",
        "        from utils import get_project_root\n",
        "        return get_project_root() + \"/\" + relativePath\n",
        "\n",
        "class CatalystDataset(Dataset):\n",
        "\n",
        "    def __init__(self, datafile='src/pl/merged_data_last29.csv'):\n",
        "        # Initialize data, download, etc.\n",
        "        # read with numpy or pandas\n",
        "        xy = np.loadtxt(resolve_path_gdrive(datafile), delimiter=',', skiprows=1, max_rows=10, dtype=float)\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "        # here the first column is the class label, the rest are the features\n",
        "        self.x_data = torch.from_numpy(xy[:,:-29])  # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(xy[:,-29:-28])  # size [n_samples, 1]\n",
        "        print(self.y_data)\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "class CatalystDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size, num_workers):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.entire_dataset = CatalystDataset()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        train_set_size = int(len(self.entire_dataset) * 0.8)\n",
        "        test_set_size = int(len(self.entire_dataset) * 0.1)\n",
        "        valid_set_size = len(self.entire_dataset) - train_set_size - test_set_size\n",
        "        self.train_ds, self.val_ds, self.test_ds = random_split(\n",
        "            self.entire_dataset, [train_set_size, valid_set_size, test_set_size])\n",
        "        return\n",
        "\n",
        "    def setup(self, stage):\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "        )\n"
      ],
      "id": "25fba0fd40809552",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-24T13:30:30.054278Z",
          "start_time": "2024-09-24T13:30:30.032219Z"
        },
        "id": "2c407a8a7a5bc7db"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "from torchmetrics import Metric\n",
        "\n",
        "\n",
        "class NN(pl.LightningModule):\n",
        "    def __init__(self, input_size, learning_rate, target=1):\n",
        "        super().__init__()\n",
        "        self.lr = learning_rate\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "        self.loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log_dict(\n",
        "            {\n",
        "                \"train_loss\": loss,\n",
        "            },\n",
        "            on_step=False,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "        )\n",
        "        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def _common_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        scores = self.forward(x)\n",
        "        loss = self.loss_fn(scores, y)\n",
        "        return loss, scores, y\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        scores = self.forward(x)\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        return preds\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2, min_lr=1e-7, verbose=True)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}"
      ],
      "id": "2c407a8a7a5bc7db",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-24T13:30:31.566760Z",
          "start_time": "2024-09-24T13:30:31.553857Z"
        },
        "id": "88eb903e9b5a6f6f"
      },
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "INPUT_SIZE = 1479\n",
        "NUM_CLASSES = 1\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "# Dataset\n",
        "DATA_DIR = \"dataset/\"\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "# Compute related\n",
        "ACCELERATOR = \"cpu\" #\"gpu\"\n",
        "DEVICES = [0]\n",
        "PRECISION = 64"
      ],
      "id": "88eb903e9b5a6f6f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-24T13:32:02.936034Z",
          "start_time": "2024-09-24T13:30:32.909398Z"
        },
        "id": "451f92a00a4137fa",
        "outputId": "8830673a-caa6-41cd-c21b-2af6577282e3",
        "colab": {
          "referenced_widgets": [
            "dbe3c89247e64a0a94abb2a107eedeac",
            "0f8d7c81ab4a452f86c493a9ec647185",
            "f3ac5416cf0a4b1eb79994942f8e1e7c",
            "89a4b3d868cd4e9e8290b3f598c184eb",
            "26552fd562524f7498e202b4fdbbea74",
            "2665a3e5347647ac8756c9b9f2ee8443",
            "7004270347984c2bac188f2dd1489e40"
          ]
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from model import NN\n",
        "from dataset import CatalystDataModule\n",
        "import config\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "torch.set_float32_matmul_precision(\"medium\") # to make lightning happy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = NN(\n",
        "        input_size=config.INPUT_SIZE,\n",
        "        learning_rate=config.LEARNING_RATE,\n",
        "        target=config.NUM_CLASSES,\n",
        "    )\n",
        "    dm = CatalystDataModule(\n",
        "        data_dir=config.DATA_DIR,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "    )\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=config.ACCELERATOR,\n",
        "        #devices=config.DEVICES,\n",
        "        min_epochs=1,\n",
        "        max_epochs=config.NUM_EPOCHS,\n",
        "        precision=config.PRECISION,\n",
        "        callbacks=[EarlyStopping(monitor=\"val_loss\")],\n",
        "    )\n",
        "    trainer.fit(model, dm)\n",
        "    trainer.validate(model, dm)\n",
        "    trainer.test(model, dm)\n"
      ],
      "id": "451f92a00a4137fa",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2489],\n",
            "        [-0.2242],\n",
            "        [-0.2304],\n",
            "        [-0.2333],\n",
            "        [-0.1994],\n",
            "        [-0.1900],\n",
            "        [-0.2272],\n",
            "        [-0.2141],\n",
            "        [-0.1572],\n",
            "        [-0.2121]], dtype=torch.float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "C:\\Users\\dalje\\.conda\\envs\\nn_310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "\n",
            "  | Name    | Type    | Params | Mode \n",
            "--------------------------------------------\n",
            "0 | fc1     | Linear  | 74.0 K | train\n",
            "1 | fc2     | Linear  | 51     | train\n",
            "2 | loss_fn | MSELoss | 0      | train\n",
            "--------------------------------------------\n",
            "74.1 K    Trainable params\n",
            "0         Non-trainable params\n",
            "74.1 K    Total params\n",
            "0.296     Total estimated model params size (MB)\n",
            "3         Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbe3c89247e64a0a94abb2a107eedeac"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dalje\\.conda\\envs\\nn_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
            "C:\\Users\\dalje\\.conda\\envs\\nn_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
            "C:\\Users\\dalje\\.conda\\envs\\nn_310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f8d7c81ab4a452f86c493a9ec647185"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3ac5416cf0a4b1eb79994942f8e1e7c"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89a4b3d868cd4e9e8290b3f598c184eb"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26552fd562524f7498e202b4fdbbea74"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2665a3e5347647ac8756c9b9f2ee8443"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
            "     Validate metric           DataLoader 0\r\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
            "        val_loss            14.076256585460625\r\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dalje\\.conda\\envs\\nn_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7004270347984c2bac188f2dd1489e40"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
            "       Test metric             DataLoader 0\r\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
            "        test_loss            568.8822369392451\r\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f9ceeceac39b7c6d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "f9ceeceac39b7c6d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}