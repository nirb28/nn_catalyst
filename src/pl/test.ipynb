{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:45:26.841200Z",
     "start_time": "2024-11-15T17:45:26.576725Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys, os\n",
    "import pytorch_lightning as pl\n",
    "import torch, math, os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Colab!\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install torchmetrics\n",
    "else:\n",
    "    print(\"Not running in Colab.\")\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "CHECKPOINTS_FOLDER = \"/checkpoints/tmp_r2\"  #stn_2_r2\"\n",
    "DEBUG = True\n",
    "seed = 42\n",
    "\n",
    "pl.seed_everything(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from sklearn.metrics import r2_score\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SequentialRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], \n",
    "                 dropout_rate: float = 0.2, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "    \n",
    "    def _compute_loss(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                      stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                     batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                       batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                  batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        # Detach tensors before storing\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y.cpu().detach(),\n",
    "            'y_pred': y_hat.cpu().detach()\n",
    "        })\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        y_true = torch.cat([out['y_true'] for out in self.test_step_outputs])\n",
    "        y_pred = torch.cat([out['y_pred'] for out in self.test_step_outputs])\n",
    "        # Detach tensors before converting to numpy\n",
    "        r2 = r2_score(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "        self.log('test_r2', r2, prog_bar=True)\n",
    "        self.test_step_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self) -> Dict:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r2 = torchmetrics.R2Score()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        accuracy = self.r2(scores, y)\n",
    "        self.log(\"train_acc\", accuracy, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        if DEBUG == True:\n",
    "            print(f\"loss: {loss}, len: {len(y)}\")\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0.000000001, threshold=0.001)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "class SingleTargetNet(BaseModel):\n",
    "\n",
    "    def __init__(self, input_size, learning_rate: float=0.001, dropout_rate: float = 0.2, target=1):\n",
    "        super(SingleTargetNet, self).__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc_skip = nn.Linear(1024, 512)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 += self.fc_skip(x1)\n",
    "\n",
    "        x3 = self.fc3(x2)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
    "                y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
    "                input_dim: int, batch_size: int = 32) -> pl.LightningModule:\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MultiTargetDataset(X_train, y_train)\n",
    "    val_dataset = MultiTargetDataset(X_val, y_val)\n",
    "    test_dataset = MultiTargetDataset(X_test, y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model with current input dimension\n",
    "    #model = SequentialRegressor(input_dim=input_dim)\n",
    "    model = SingleTargetNet (\n",
    "        input_size=input_dim\n",
    "    )    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        #EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "        ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ]\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=150,\n",
    "        callbacks=callbacks,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=True,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train and test the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_training(df: pd.DataFrame, num_features: int = 1479, \n",
    "                       num_targets: int = 29, \n",
    "                       target_range: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n",
    "    X = df.iloc[:, :num_features].values\n",
    "    y = df.iloc[:, num_features:num_features+num_targets].values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, shuffle=False)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create separate scalers for each target\n",
    "    y_scalers = [StandardScaler() for _ in range(num_targets)]\n",
    "    y_train_scaled = np.zeros_like(y_train)\n",
    "    y_val_scaled = np.zeros_like(y_val)\n",
    "    y_test_scaled = np.zeros_like(y_test)\n",
    "    \n",
    "    # Scale each target separately\n",
    "    for i in range(num_targets):\n",
    "        y_train_scaled[:, i] = y_scalers[i].fit_transform(y_train[:, i].reshape(-1, 1)).ravel()\n",
    "        y_val_scaled[:, i] = y_scalers[i].transform(y_val[:, i].reshape(-1, 1)).ravel()\n",
    "        y_test_scaled[:, i] = y_scalers[i].transform(y_test[:, i].reshape(-1, 1)).ravel()\n",
    "    \n",
    "    final_predictions = pd.DataFrame()\n",
    "    \n",
    "    # Initialize current features for each dataset\n",
    "    current_train_features = X_train_scaled.copy()\n",
    "    current_val_features = X_val_scaled.copy()\n",
    "    current_test_features = X_test_scaled.copy()\n",
    "    \n",
    "    r2_scores = []\n",
    "    \n",
    "    if target_range:\n",
    "        start, end = target_range\n",
    "    else:\n",
    "        start, end = 0, num_targets\n",
    "    \n",
    "    for target_idx in range(start, end):\n",
    "        print(f\"\\nTraining model for target {target_idx + 1}/{end - start}\")\n",
    "        \n",
    "        # Get current target\n",
    "        current_target = y_train_scaled[:, target_idx].reshape(-1, 1)\n",
    "        current_val_target = y_val_scaled[:, target_idx].reshape(-1, 1)\n",
    "        current_test_target = y_test_scaled[:, target_idx].reshape(-1, 1)\n",
    "        \n",
    "        # Create model with current input dimension\n",
    "        current_input_dim = current_train_features.shape[1]\n",
    "        \n",
    "        model = train_model(\n",
    "            current_train_features,\n",
    "            current_val_features,\n",
    "            current_test_features,\n",
    "            current_target,\n",
    "            current_val_target,\n",
    "            current_test_target,\n",
    "            input_dim=current_input_dim  # Pass current input dimension\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Make predictions for training set\n",
    "            train_predictions = model(torch.tensor(current_train_features, dtype=torch.float32))\n",
    "            train_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
    "                train_predictions.cpu().detach().numpy()\n",
    "            )\n",
    "            \n",
    "            # Make predictions for validation and test sets\n",
    "            val_predictions = model(torch.tensor(current_val_features, dtype=torch.float32))\n",
    "            test_predictions = model(torch.tensor(current_test_features, dtype=torch.float32))\n",
    "            \n",
    "            # Store predictions for current target\n",
    "            final_predictions[f'target_{target_idx+1}_pred'] = train_predictions_orig.flatten()\n",
    "            \n",
    "            # Update features for next iteration\n",
    "            current_train_features = np.hstack([\n",
    "                current_train_features, \n",
    "                train_predictions.cpu().detach().numpy()\n",
    "            ])\n",
    "            current_val_features = np.hstack([\n",
    "                current_val_features, \n",
    "                val_predictions.cpu().detach().numpy()\n",
    "            ])\n",
    "            current_test_features = np.hstack([\n",
    "                current_test_features, \n",
    "                test_predictions.cpu().detach().numpy()\n",
    "            ])\n",
    "        \n",
    "        # Calculate and store R2 score\n",
    "        test_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
    "            test_predictions.cpu().detach().numpy()\n",
    "        )\n",
    "        r2 = r2_score(y_test[:, target_idx], test_predictions_orig.flatten())\n",
    "        r2_scores.append(r2)\n",
    "        print(f\"R2 score for target {target_idx + 1}: {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal R2 scores for all targets:\")\n",
    "    for i, r2 in enumerate(r2_scores, start=1):\n",
    "        print(f\"Target {i}: {r2:.4f}\")\n",
    "    \n",
    "    return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "datafile='src/pl/merged_data_last29_reordered_byR2.csv'\n",
    "max_rows=None\n",
    "df = pd.read_csv(resolve_path_gdrive(datafile), delimiter=',', skiprows=0, dtype=float, nrows=max_rows)\n",
    "\n",
    "# Run sequential training\n",
    "predictions = sequential_training(df)\n",
    "# Save predictions\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the file containing target and predicted values\n",
    "file_path = 'path_to_your_file.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Adjusting the code to match the name pattern 'target_1_pred', 'target_1'\n",
    "r2_scores = []\n",
    "\n",
    "for i in range(1, (len(data.columns) // 2) + 1):\n",
    "    targets = data[f'target_{i}']\n",
    "    predictions = data[f'target_{i}_pred']\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Create a DataFrame to tabulate the results\n",
    "results = pd.DataFrame({\n",
    "    'Target': [f'target_{i}' for i in range(1, (len(data.columns) // 2) + 1)],\n",
    "    'R2 Score': r2_scores\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
