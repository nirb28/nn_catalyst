{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T00:03:38.133483Z",
     "start_time": "2024-11-15T00:03:28.234685Z"
    }
   },
   "source": [
    "%%capture\n",
    "import sys, os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Colab!\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install torchmetrics\n",
    "else:\n",
    "    print(\"Not running in Colab.\")\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "CHECKPOINTS_FOLDER = \"/checkpoints/tmp_r1\"  #stn_2_r2\"\n",
    "import pytorch_lightning as pl\n",
    "import torch, math, os\n",
    "\n",
    "seed = 42\n",
    "pl.seed_everything(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T00:04:06.231156Z",
     "start_time": "2024-11-15T00:04:04.594449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from sklearn.metrics import r2_score\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SequentialRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], \n",
    "                 dropout_rate: float = 0.2, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "    \n",
    "    def _compute_loss(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                      stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                     batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                       batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                  batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        # Detach tensors before storing\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y.cpu().detach(),\n",
    "            'y_pred': y_hat.cpu().detach()\n",
    "        })\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        y_true = torch.cat([out['y_true'] for out in self.test_step_outputs])\n",
    "        y_pred = torch.cat([out['y_pred'] for out in self.test_step_outputs])\n",
    "        # Detach tensors before converting to numpy\n",
    "        r2 = r2_score(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "        self.log('test_r2', r2, prog_bar=True)\n",
    "        self.test_step_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self) -> Dict:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "def train_model(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
    "                y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
    "                input_dim: int, batch_size: int = 32) -> pl.LightningModule:\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MultiTargetDataset(X_train, y_train)\n",
    "    val_dataset = MultiTargetDataset(X_val, y_val)\n",
    "    test_dataset = MultiTargetDataset(X_test, y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model with current input dimension\n",
    "    model = SequentialRegressor(input_dim=input_dim)\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "        ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ]\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=150,\n",
    "        callbacks=callbacks,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=True,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train and test the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-15T00:04:10.195545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequential_training(df: pd.DataFrame, num_features: int = 1479, \n",
    "                       num_targets: int = 29, \n",
    "                       target_range: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n",
    "    X = df.iloc[:, :num_features].values\n",
    "    y = df.iloc[:, num_features:num_features+num_targets].values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, shuffle=False)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create separate scalers for each target\n",
    "    y_scalers = [StandardScaler() for _ in range(num_targets)]\n",
    "    y_train_scaled = np.zeros_like(y_train)\n",
    "    y_val_scaled = np.zeros_like(y_val)\n",
    "    y_test_scaled = np.zeros_like(y_test)\n",
    "    \n",
    "    # Scale each target separately\n",
    "    for i in range(num_targets):\n",
    "        y_train_scaled[:, i] = y_scalers[i].fit_transform(y_train[:, i].reshape(-1, 1)).ravel()\n",
    "        y_val_scaled[:, i] = y_scalers[i].transform(y_val[:, i].reshape(-1, 1)).ravel()\n",
    "        y_test_scaled[:, i] = y_scalers[i].transform(y_test[:, i].reshape(-1, 1)).ravel()\n",
    "    \n",
    "    final_predictions = pd.DataFrame()\n",
    "    \n",
    "    # Initialize current features for each dataset\n",
    "    current_train_features = X_train_scaled.copy()\n",
    "    current_val_features = X_val_scaled.copy()\n",
    "    current_test_features = X_test_scaled.copy()\n",
    "    \n",
    "    r2_scores = []\n",
    "    \n",
    "    if target_range:\n",
    "        start, end = target_range\n",
    "    else:\n",
    "        start, end = 0, num_targets\n",
    "    \n",
    "    for target_idx in range(start, end):\n",
    "        print(f\"\\nTraining model for target {target_idx + 1}/{end - start}\")\n",
    "        \n",
    "        # Get current target\n",
    "        current_target = y_train_scaled[:, target_idx].reshape(-1, 1)\n",
    "        current_val_target = y_val_scaled[:, target_idx].reshape(-1, 1)\n",
    "        current_test_target = y_test_scaled[:, target_idx].reshape(-1, 1)\n",
    "        \n",
    "        # Create model with current input dimension\n",
    "        current_input_dim = current_train_features.shape[1]\n",
    "        \n",
    "        model = train_model(\n",
    "            current_train_features,\n",
    "            current_val_features,\n",
    "            current_test_features,\n",
    "            current_target,\n",
    "            current_val_target,\n",
    "            current_test_target,\n",
    "            input_dim=current_input_dim  # Pass current input dimension\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Make predictions for training set\n",
    "            train_predictions = model(torch.tensor(current_train_features, dtype=torch.float32))\n",
    "            train_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
    "                train_predictions.cpu().detach().numpy()\n",
    "            )\n",
    "            \n",
    "            # Make predictions for validation and test sets\n",
    "            val_predictions = model(torch.tensor(current_val_features, dtype=torch.float32))\n",
    "            test_predictions = model(torch.tensor(current_test_features, dtype=torch.float32))\n",
    "            \n",
    "            # Store predictions for current target\n",
    "            final_predictions[f'target_{target_idx+1}_pred'] = train_predictions_orig.flatten()\n",
    "            \n",
    "            # Update features for next iteration\n",
    "            current_train_features = np.hstack([\n",
    "                current_train_features, \n",
    "                train_predictions.cpu().detach().numpy()\n",
    "            ])\n",
    "            current_val_features = np.hstack([\n",
    "                current_val_features, \n",
    "                val_predictions.cpu().detach().numpy()\n",
    "            ])\n",
    "            current_test_features = np.hstack([\n",
    "                current_test_features, \n",
    "                test_predictions.cpu().detach().numpy()\n",
    "            ])\n",
    "        \n",
    "        # Calculate and store R2 score\n",
    "        test_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
    "            test_predictions.cpu().detach().numpy()\n",
    "        )\n",
    "        r2 = r2_score(y_test[:, target_idx], test_predictions_orig.flatten())\n",
    "        r2_scores.append(r2)\n",
    "        print(f\"R2 score for target {target_idx + 1}: {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal R2 scores for all targets:\")\n",
    "    for i, r2 in enumerate(r2_scores, start=1):\n",
    "        print(f\"Target {i}: {r2:.4f}\")\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "datafile='src/pl/merged_data_last29_reordered_byR2.csv'\n",
    "max_rows=200\n",
    "df = pd.read_csv(resolve_path_gdrive(datafile), delimiter=',', skiprows=0, dtype=float, nrows=max_rows)\n",
    "# Run sequential training\n",
    "predictions = sequential_training(df)\n",
    "# Save predictions\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for target 1/29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c62dc13cd57488ebd123e1e1c8def3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80fe62fac0ff43349dbf9ced052bcfb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e1a4b5b53144f78914ec91a1ae3964b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a83d36e454e240c296711a28f5b18308"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81323dbf19894ffe93597fd761babd23"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cce1b6a43a6d42a5918a2997098086bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c91d216b715d4bd4b7625c58ddb5db9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66a010d301ef4fda91203e9835dfdf95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b91747c816d54b488187d007126afe0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cd508085bb64ff78dfa8c28e83da56d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29cc4b2e69f84d499e2e71f079372181"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c4cec1944f245388e9eadbdf275864c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4278dcc75dcf4936bc6cd9e389114158"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fea6002cec79482eb0093797b892c77e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b16db654049045a5be2f83f8c5865bd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d359813fc56464ab7e9e9d793c37ebd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39996c2ec4a74de2a00731616596ca96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a3a3f34f1274653b166404ad6068212"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2deab2710274fbbbd7b97fedf975928"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1dcbf3957ce344e294e843f0b2b4c408"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6618883013725281    \u001B[0m\u001B[35m \u001B[0m│\n",
       "│\u001B[36m \u001B[0m\u001B[36m         test_r2         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.8300211429595947    \u001B[0m\u001B[35m \u001B[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6618883013725281     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8300211429595947     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for target 1: 0.8300\n",
      "\n",
      "Training model for target 2/29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bde41147f3564738844f88c35b6a7874"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06def1d782b54116bc088c537fdc7a2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8eeb2070e0b14a13b5545ceb6116a87b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49dabcfd1a6848c790e5c05f0f8c2e31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f78f21ac3a9b4a61b0acca8f48ba05df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fee46d8abfa841a9abd7a2488ee8674f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e14f49ee0be84a49854009d39d01fc24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d63b316628d9420fb3955e70ff410c73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "090699d1020b41b9b9595f8843e6424e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4728ebf6da5a4028ac327773533b19ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d7581335bae443fb266a0932bf9f7c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd5af1a362ca46b682c6023952e0f735"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c605fc6c6d646538f88c6f2be6c0a78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91e19456f6c94449bd0b8f3d99fd3e28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.8560398817062378    \u001B[0m\u001B[35m \u001B[0m│\n",
       "│\u001B[36m \u001B[0m\u001B[36m         test_r2         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.7801651954650879    \u001B[0m\u001B[35m \u001B[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8560398817062378     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7801651954650879     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 114\u001B[0m\n\u001B[0;32m    112\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(resolve_path_gdrive(datafile), delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m, skiprows\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m, nrows\u001B[38;5;241m=\u001B[39mmax_rows)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# Run sequential training\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43msequential_training\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;66;03m# Save predictions\u001B[39;00m\n\u001B[0;32m    116\u001B[0m predictions\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredictions.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[3], line 95\u001B[0m, in \u001B[0;36msequential_training\u001B[1;34m(df, num_features, num_targets, target_range)\u001B[0m\n\u001B[0;32m     89\u001B[0m     current_test_features \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack([\n\u001B[0;32m     90\u001B[0m         current_test_features, \n\u001B[0;32m     91\u001B[0m         test_predictions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     92\u001B[0m     ])\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# Calculate and store R2 score\u001B[39;00m\n\u001B[1;32m---> 95\u001B[0m test_predictions_orig \u001B[38;5;241m=\u001B[39m \u001B[43my_scalers\u001B[49m[target_idx]\u001B[38;5;241m.\u001B[39minverse_transform(\n\u001B[0;32m     96\u001B[0m     test_predictions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     97\u001B[0m )\n\u001B[0;32m     98\u001B[0m r2 \u001B[38;5;241m=\u001B[39m r2_score(y_test[:, target_idx], test_predictions_orig\u001B[38;5;241m.\u001B[39mflatten())\n\u001B[0;32m     99\u001B[0m r2_scores\u001B[38;5;241m.\u001B[39mappend(r2)\n",
      "Cell \u001B[1;32mIn[3], line 95\u001B[0m, in \u001B[0;36msequential_training\u001B[1;34m(df, num_features, num_targets, target_range)\u001B[0m\n\u001B[0;32m     89\u001B[0m     current_test_features \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack([\n\u001B[0;32m     90\u001B[0m         current_test_features, \n\u001B[0;32m     91\u001B[0m         test_predictions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     92\u001B[0m     ])\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# Calculate and store R2 score\u001B[39;00m\n\u001B[1;32m---> 95\u001B[0m test_predictions_orig \u001B[38;5;241m=\u001B[39m \u001B[43my_scalers\u001B[49m[target_idx]\u001B[38;5;241m.\u001B[39minverse_transform(\n\u001B[0;32m     96\u001B[0m     test_predictions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     97\u001B[0m )\n\u001B[0;32m     98\u001B[0m r2 \u001B[38;5;241m=\u001B[39m r2_score(y_test[:, target_idx], test_predictions_orig\u001B[38;5;241m.\u001B[39mflatten())\n\u001B[0;32m     99\u001B[0m r2_scores\u001B[38;5;241m.\u001B[39mappend(r2)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2024.1.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1196\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1193\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1196\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2024.1.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1211\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1208\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1210\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1211\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1213\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1215\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
