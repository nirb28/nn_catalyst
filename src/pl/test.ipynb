{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:40:09.450248Z",
     "start_time": "2024-11-17T04:40:04.771806Z"
    }
   },
   "source": [
    "%%capture\n",
    "import sys, os\n",
    "import pytorch_lightning as pl\n",
    "import torch, math, os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Colab!\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install torchmetrics\n",
    "else:\n",
    "    print(\"Not running in Colab.\")\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "CHECKPOINTS_FOLDER_BASE = \"/checkpoints/stn_r1\"\n",
    "CHECKPOINTS_FOLDER = CHECKPOINTS_FOLDER_BASE\n",
    "DEBUG = False\n",
    "seed = 42\n",
    "\n",
    "pl.seed_everything(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:40:10.479948Z",
     "start_time": "2024-11-17T04:40:09.452248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from sklearn.metrics import r2_score\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:40:16.948113Z",
     "start_time": "2024-11-17T04:40:16.934564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn, optim\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r2 = torchmetrics.R2Score()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        accuracy = self.r2(scores, y)\n",
    "        self.log(\"train_acc\", accuracy, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        if DEBUG == True:\n",
    "            print(f\"loss: {loss}, len: {len(y)}\")\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0.000000001, threshold=0.001)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "class SingleTargetNet(BaseModel):\n",
    "\n",
    "    def __init__(self, input_size, learning_rate: float=0.001, dropout_rate: float = 0.2, target=1):\n",
    "        super(SingleTargetNet, self).__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc_skip = nn.Linear(1024, 512)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 += self.fc_skip(x1)\n",
    "\n",
    "        x3 = self.fc3(x2)\n",
    "        return x3"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:40:33.990330Z",
     "start_time": "2024-11-17T04:40:33.976331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
    "                y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
    "                input_dim: int, target_num, batch_size: int = 32) -> pl.LightningModule:\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MultiTargetDataset(X_train, y_train)\n",
    "    val_dataset = MultiTargetDataset(X_val, y_val)\n",
    "    test_dataset = MultiTargetDataset(X_test, y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model with current input dimension\n",
    "    #model = SequentialRegressor(input_dim=input_dim)\n",
    "    model = SingleTargetNet (\n",
    "        input_size=input_dim\n",
    "    )    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=True), #monitor=\"train_loss\"\n",
    "        ModelCheckpoint(\n",
    "            dirpath=resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/{target_num}'),\n",
    "            filename='{epoch:02d}-{val_loss:.2f}',\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min'\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ]\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=150,\n",
    "        callbacks=callbacks,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=True,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train and test the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:40:43.677172Z",
     "start_time": "2024-11-17T04:40:43.649031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequential_training(df: pd.DataFrame, num_features: int = 1479, \n",
    "                       num_targets: int = 29, stack_predictions = True, scale_y = True,\n",
    "                       target_range: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n",
    "    X = df.iloc[:, :num_features].values\n",
    "    y = df.iloc[:, num_features:num_features+num_targets].values\n",
    "    global CHECKPOINTS_FOLDER \n",
    "    CHECKPOINTS_FOLDER = f\"{CHECKPOINTS_FOLDER_BASE}/stack={stack_predictions}-scaleY={scale_y}\"\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, shuffle=True)\n",
    "    \n",
    "    print(f\"Sizes: {X_train.shape}, {X_val.shape}, {X_test.shape}, {y_train.shape}, {y_val.shape}\")\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    if scale_y:\n",
    "        # Create separate scalers for each target\n",
    "        y_scalers = [StandardScaler() for _ in range(num_targets)]\n",
    "        y_train_scaled = np.zeros_like(y_train)\n",
    "        y_val_scaled = np.zeros_like(y_val)\n",
    "        y_test_scaled = np.zeros_like(y_test)\n",
    "        \n",
    "        # Scale each target separately\n",
    "        for i in range(num_targets):\n",
    "            y_train_scaled[:, i] = y_scalers[i].fit_transform(y_train[:, i].reshape(-1, 1)).ravel()\n",
    "            y_val_scaled[:, i] = y_scalers[i].transform(y_val[:, i].reshape(-1, 1)).ravel()\n",
    "            y_test_scaled[:, i] = y_scalers[i].transform(y_test[:, i].reshape(-1, 1)).ravel()\n",
    "    \n",
    "    final_predictions = pd.DataFrame()\n",
    "    \n",
    "    # Initialize current features for each dataset\n",
    "    current_train_features = X_train_scaled.copy()\n",
    "    current_val_features = X_val_scaled.copy()\n",
    "    current_test_features = X_test_scaled.copy()\n",
    "    \n",
    "    r2_scores = []\n",
    "    \n",
    "    if target_range:\n",
    "        start, end = target_range\n",
    "    else:\n",
    "        start, end = 0, num_targets\n",
    "    \n",
    "    for target_idx in range(start, end):\n",
    "        print(f\"\\nTraining model for target {target_idx + 1}/{end - start}. Features: {current_train_features.shape[1]}\")\n",
    "        # Get current target        \n",
    "        if scale_y:\n",
    "            current_target = y_train_scaled[:, target_idx].reshape(-1, 1)\n",
    "            current_val_target = y_val_scaled[:, target_idx].reshape(-1, 1)\n",
    "            current_test_target = y_test_scaled[:, target_idx].reshape(-1, 1)\n",
    "        else:\n",
    "            current_target = y_train[:, target_idx].reshape(-1, 1)\n",
    "            current_val_target = y_val[:, target_idx].reshape(-1, 1)\n",
    "            current_test_target = y_test[:, target_idx].reshape(-1, 1)\n",
    "                \n",
    "        # Create model with current input dimension\n",
    "        current_input_dim = current_train_features.shape[1]\n",
    "        \n",
    "        model = train_model(\n",
    "            current_train_features,\n",
    "            current_val_features,\n",
    "            current_test_features,\n",
    "            current_target,\n",
    "            current_val_target,\n",
    "            current_test_target,\n",
    "            input_dim=current_input_dim,\n",
    "            target_num=target_idx + 1\n",
    "        )\n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Make predictions for training set\n",
    "            train_predictions = model(torch.tensor(current_train_features, dtype=torch.float32))\n",
    "            \n",
    "            # Make predictions for validation and test sets\n",
    "            val_predictions = model(torch.tensor(current_val_features, dtype=torch.float32))\n",
    "            test_predictions = model(torch.tensor(current_test_features, dtype=torch.float32))\n",
    "            \n",
    "            all_predictions = torch.cat((train_predictions, val_predictions, test_predictions)).cpu().detach().numpy()\n",
    "            all_targets = np.concatenate((current_target, current_val_target, current_test_target))\n",
    "            # Store predictions for current target\n",
    "            # final_predictions[f'train_target_{target_idx+1}_pred'] = train_predictions.flatten()\n",
    "            # final_predictions[f'train_target_{target_idx+1}'] = current_target\n",
    "            # \n",
    "            # final_predictions[f'val_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
    "            # final_predictions[f'val_target_{target_idx+1}'] = current_val_target\n",
    "            # \n",
    "            # final_predictions[f'test_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
    "            # final_predictions[f'test_target_{target_idx+1}'] = current_test_target\n",
    "            \n",
    "            final_predictions[f'all_target_{target_idx+1}_pred'] = all_predictions.flatten()\n",
    "            final_predictions[f'all_target_{target_idx+1}'] = all_targets\n",
    "\n",
    "            if stack_predictions == True:\n",
    "                # Update features for next iteration\n",
    "                current_train_features = np.hstack([\n",
    "                    current_train_features, \n",
    "                    train_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "                current_val_features = np.hstack([\n",
    "                    current_val_features, \n",
    "                    val_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "                current_test_features = np.hstack([\n",
    "                    current_test_features, \n",
    "                    test_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "        \n",
    "        # Calculate and store R2 score\n",
    "        if scale_y:\n",
    "            test_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
    "                test_predictions.cpu().detach().numpy()\n",
    "            )            \n",
    "            r2 = r2_score(y_test[:, target_idx], test_predictions_orig.flatten())\n",
    "        else:\n",
    "            r2 = r2_score(y_test[:, target_idx], test_predictions.flatten())\n",
    "            \n",
    "        r2_scores.append(r2)\n",
    "        print(f\"R2 score for target {target_idx + 1}: {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal R2 scores for all targets:\")\n",
    "    for i, r2 in enumerate(r2_scores, start=1):\n",
    "        print(f\"Target {i}: {r2:.4f}\")\n",
    "    \n",
    "    return final_predictions\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:40:58.256791Z",
     "start_time": "2024-11-17T04:40:53.395249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "datafile='src/pl/merged_data_last29_reordered_byR2.csv'\n",
    "max_rows=None\n",
    "df = pd.read_csv(resolve_path_gdrive(datafile), delimiter=',', skiprows=0, dtype=float, nrows=max_rows)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-17T04:41:02.065658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run sequential training\n",
    "predictions = sequential_training(df, stack_predictions=True, scale_y=True, target_range=None)\n",
    "# Save predictions\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: (15739, 1479), (5247, 1479), (5247, 1479), (15739, 29), (5247, 29)\n",
      "\n",
      "Training model for target 1/29. Features: 1479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "503256ebc6bc4760a9a5e6b9145a0244"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ds\\work\\utilities\\conda\\envs\\nn_310_2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "d:\\ds\\work\\utilities\\conda\\envs\\nn_310_2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9047ec3671884abd8aecdf1cb5271715"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "066f0225d0214c11afdfb2ffe7a01cff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af5f063699934a45b6dcab0a03e613fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d90c495b451144f3a0113999eb109d2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81a35557ea58423691ea007a9209cae7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88e91672669f407faab9e019c7ca51a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52f327e2d8d2421b9a2c986fac9260e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "289fe634a41a4ea1bbdebf4df4ed8e84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model: stacking, test r2, 150 epoch\n",
    "Final R2 scores for all targets:\n",
    "Target 1: 0.9889\n",
    "Target 2: 0.9922\n",
    "Target 3: 0.9900\n",
    "Target 4: 0.9893\n",
    "Target 5: 0.9901\n",
    "Target 6: 0.9879\n",
    "Target 7: 0.9669\n",
    "Target 8: 0.9625\n",
    "Target 9: 0.9608\n",
    "Target 10: 0.9448\n",
    "Target 11: 0.9306\n",
    "Target 12: 0.9446\n",
    "Target 13: 0.9376\n",
    "Target 14: 0.9301\n",
    "Target 15: 0.9415\n",
    "Target 16: 0.9219\n",
    "Target 17: 0.8800\n",
    "Target 18: 0.8838\n",
    "Target 19: 0.8417\n",
    "Target 20: 0.8386\n",
    "Target 21: 0.8452\n",
    "Target 22: 0.8136\n",
    "Target 23: 0.7956\n",
    "Target 24: 0.7864\n",
    "Target 25: 0.6782\n",
    "Target 26: 0.6834\n",
    "Target 27: 0.5847\n",
    "Target 28: 0.5586\n",
    "Target 29: 0.4338"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:39:09.803323Z",
     "start_time": "2024-11-17T04:39:09.498226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the file containing target and predicted values\n",
    "file_path = '/checkpoints/stn_r1/stack=True-scaleY=True/predictions.csv'\n",
    "data = pd.read_csv(resolve_path_gdrive(file_path))\n",
    "\n",
    "# Adjusting the code to match the name pattern 'target_1_pred', 'target_1'\n",
    "r2_scores = []\n",
    "[train, test, val] = [15739, 5247, 5247]\n",
    "for i in range(1, (len(data.columns) // 2) + 1):\n",
    "    train_targets = data[f'all_target_{i}'][:train]\n",
    "    train_predictions = data[f'all_target_{i}_pred'][:train]\n",
    "    val_targets = data[f'all_target_{i}'][train:]\n",
    "    val_predictions = data[f'all_target_{i}_pred'][train:]\n",
    "    test_targets = data[f'all_target_{i}'][train+test:]\n",
    "    test_predictions = data[f'all_target_{i}_pred'][train+test:]\n",
    "    \n",
    "    r2 = [r2_score(train_targets, train_predictions), r2_score(val_targets, val_predictions), r2_score(test_targets, test_predictions)]\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Create a DataFrame to tabulate the results\n",
    "results = pd.DataFrame({\n",
    "    'Target': [f'target_{i}' for i in range(1, (len(data.columns) // 2) + 1)],\n",
    "    'R2 Score [train, val, test]': r2_scores\n",
    "})\n",
    "\n",
    "print(results)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Target                        R2 Score [train, val, test]\n",
      "0    target_1  [0.9824148100660581, 0.9780502984742219, 0.972...\n",
      "1    target_2  [0.8979788138926885, 0.8929131427514477, 0.887...\n",
      "2    target_3  [0.9737432100223783, 0.965676295177207, 0.9575...\n",
      "3    target_4  [0.9770670726986855, 0.973489571271416, 0.9711...\n",
      "4    target_5  [0.9785964303605776, 0.9759807859825803, 0.975...\n",
      "5    target_6  [0.9510843219690669, 0.9478761020596811, 0.947...\n",
      "6    target_7  [0.9362204056829968, 0.9341844279193448, 0.934...\n",
      "7    target_8  [0.9442090160299694, 0.9346594316831095, 0.932...\n",
      "8    target_9  [0.9326175059648436, 0.9258850839999507, 0.926...\n",
      "9   target_10  [0.9146841886645203, 0.8945233473429507, 0.894...\n",
      "10  target_11  [0.9061511156763594, 0.8779621690229521, 0.881...\n",
      "11  target_12  [0.9277067622544798, 0.907809835637711, 0.9164...\n",
      "12  target_13  [0.9090101159999711, 0.8964180816937394, 0.897...\n",
      "13  target_14  [0.8816674501433857, 0.8779152018561418, 0.873...\n",
      "14  target_15  [0.898695291280698, 0.8903818217820029, 0.8898...\n",
      "15  target_16  [0.8801044651993155, 0.8571607211358064, 0.855...\n",
      "16  target_17  [0.9110826002777558, 0.8939385091094318, 0.889...\n",
      "17  target_18  [0.8558750106228125, 0.822968403333791, 0.8170...\n",
      "18  target_19  [0.8471210087411343, 0.8066419682581313, 0.800...\n",
      "19  target_20  [0.8291934047670156, 0.7895068270682646, 0.793...\n",
      "20  target_21  [0.7698277149996271, 0.7511198355345678, 0.742...\n",
      "21  target_22  [0.8206662631827838, 0.7621675455999795, 0.757...\n",
      "22  target_23  [0.7965283565530248, 0.7419044939755202, 0.736...\n",
      "23  target_24  [0.765510392899982, 0.6457470552761588, 0.6051...\n",
      "24  target_25  [0.6374693150353106, 0.591860688160446, 0.6020...\n",
      "25  target_26  [0.7049497123557215, 0.6136389751055409, 0.616...\n",
      "26  target_27  [0.5795401518225932, 0.4447844650258076, 0.468...\n",
      "27  target_28  [0.5665231085701854, 0.4491156979254255, 0.441...\n",
      "28  target_29  [0.4507509785871002, 0.3935315842967415, 0.382...\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SequentialRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], \n",
    "                 dropout_rate: float = 0.2, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "    \n",
    "    def _compute_loss(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                      stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                     batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                       batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                  batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        # Detach tensors before storing\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y.cpu().detach(),\n",
    "            'y_pred': y_hat.cpu().detach()\n",
    "        })\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        y_true = torch.cat([out['y_true'] for out in self.test_step_outputs])\n",
    "        y_pred = torch.cat([out['y_pred'] for out in self.test_step_outputs])\n",
    "        # Detach tensors before converting to numpy\n",
    "        r2 = r2_score(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "        self.log('test_r2', r2, prog_bar=True)\n",
    "        self.test_step_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self) -> Dict:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
