{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:38:24.636169Z",
     "start_time": "2024-11-14T19:38:24.546238Z"
    }
   },
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Colab!\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install torchmetrics\n",
    "else:\n",
    "    print(\"Not running in Colab.\")\n",
    "\n",
    "CHECKPOINTS_FOLDER = \"/checkpoints/tmp_r1\"  #stn_2_r2\"\n",
    "import pytorch_lightning as pl\n",
    "import torch, math, os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "pl.seed_everything(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:38:27.866850Z",
     "start_time": "2024-11-14T19:38:27.827209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self, X, y, seed=42):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.seed = seed\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        random.seed(self.seed + idx)\n",
    "        np.random.seed(self.seed + idx)\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SequentialRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, learning_rate=0.001, seed=42):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        \n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return {'y_true': y.detach().cpu(), 'y_pred': y_hat.detach().cpu()}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        y_true = torch.cat([out['y_true'] for out in outputs])\n",
    "        y_pred = torch.cat([out['y_pred'] for out in outputs])\n",
    "        r2 = r2_score(y_true.numpy(), y_pred.numpy())\n",
    "        self.log('test_r2', r2)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "\n",
    "def train_single_target(X_train, X_val, X_test, y_train, y_val, y_test, input_dim, seed=42):\n",
    "    train_dataset = MultiTargetDataset(X_train, y_train, seed=seed)\n",
    "    val_dataset = MultiTargetDataset(X_val, y_val, seed=seed)\n",
    "    test_dataset = MultiTargetDataset(X_test, y_test, seed=seed)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    model = SequentialRegressor(input_dim, seed=seed)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        callbacks=[early_stopping, checkpoint_callback, lr_monitor],\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        fast_dev_run=True  # New parameter\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def sequential_training(df, num_features=1479, num_targets=29, target_range=None, seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = df.iloc[:, :num_features].values\n",
    "    y = df.iloc[:, num_features:num_features+num_targets].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    y_scaler = StandardScaler()\n",
    "    y_train = y_scaler.fit_transform(y_train)\n",
    "    y_val = y_scaler.transform(y_val)\n",
    "    y_test = y_scaler.transform(y_test)\n",
    "    \n",
    "    final_predictions = pd.DataFrame()\n",
    "    current_features = X_train.copy()\n",
    "    \n",
    "    r2_scores = []\n",
    "    \n",
    "    if target_range:\n",
    "        start, end = target_range\n",
    "    else:\n",
    "        start, end = 0, num_targets\n",
    "    \n",
    "    for target_idx in range(start, end):\n",
    "        print(f\"\\nTraining model for target {target_idx + 1}/{end - start}\")\n",
    "        \n",
    "        current_target = y_train[:, target_idx].reshape(-1, 1)\n",
    "        \n",
    "        model = train_single_target(\n",
    "            current_features, \n",
    "            X_val, \n",
    "            X_test,\n",
    "            current_target, \n",
    "            y_val[:, target_idx].reshape(-1, 1), \n",
    "            y_test[:, target_idx].reshape(-1, 1),\n",
    "            input_dim=current_features.shape[1],\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(torch.tensor(current_features, dtype=torch.float32))\n",
    "            predictions = y_scaler.inverse_transform(predictions.numpy())\n",
    "        \n",
    "        final_predictions[f'target_{target_idx+1}_pred'] = predictions.flatten()\n",
    "        \n",
    "        current_features = np.hstack([current_features, predictions])\n",
    "        \n",
    "        # Calculate R2 score for current target\n",
    "        r2 = r2_score(y_test[:, target_idx], predictions.flatten())\n",
    "        r2_scores.append(r2)\n",
    "        \n",
    "    print(\"\\nR2 scores for all targets on test set:\")\n",
    "    for i, r2 in enumerate(r2_scores):\n",
    "        print(f\"Target {i+1}: {r2:.2f}\")\n",
    "    \n",
    "    # Load best checkpoint and calculate R2 score on full dataset\n",
    "    model = SequentialRegressor(X.shape[1], seed=seed)\n",
    "    trainer = pl.Trainer(seed=seed)\n",
    "    model = trainer.load_from_checkpoint(checkpoint_path=checkpoint_callback.best_model_path)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        full_predictions = model(torch.tensor(X, dtype=torch.float32))\n",
    "        full_predictions = y_scaler.inverse_transform(full_predictions.numpy())\n",
    "    \n",
    "    full_r2 = r2_score(y, full_predictions)\n",
    "    print(f\"\\nR2 score on full dataset: {full_r2:.2f}\")\n",
    "    \n",
    "    return final_predictions\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:39:39.104802Z",
     "start_time": "2024-11-14T19:39:38.906068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "datafile='src/pl/merged_data_last29_reordered_byR2.csv'\n",
    "max_rows=200\n",
    "df = pd.read_csv(resolve_path_gdrive(datafile), delimiter=',', skiprows=1, dtype=float, nrows=max_rows)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:39:47.179679Z",
     "start_time": "2024-11-14T19:39:46.931132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run sequential training\n",
    "predictions = sequential_training(df)\n",
    "# Save predictions\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for target 1/29\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Run sequential training\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43msequential_training\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Save predictions\u001B[39;00m\n\u001B[0;32m      4\u001B[0m predictions\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredictions.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[6], line 160\u001B[0m, in \u001B[0;36msequential_training\u001B[1;34m(df, num_features, num_targets, target_range, seed)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTraining model for target \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_idx\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mend\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mstart\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    158\u001B[0m current_target \u001B[38;5;241m=\u001B[39m y_train[:, target_idx]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 160\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_single_target\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcurrent_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcurrent_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurrent_features\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "Cell \u001B[1;32mIn[6], line 109\u001B[0m, in \u001B[0;36mtrain_single_target\u001B[1;34m(X_train, X_val, X_test, y_train, y_val, y_test, input_dim, seed)\u001B[0m\n\u001B[0;32m    106\u001B[0m checkpoint_callback \u001B[38;5;241m=\u001B[39m ModelCheckpoint(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m, save_top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    107\u001B[0m lr_monitor \u001B[38;5;241m=\u001B[39m LearningRateMonitor(logging_interval\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 109\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpoint_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_monitor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_model_summary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfast_dev_run\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# New parameter\u001B[39;49;00m\n\u001B[0;32m    118\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model, train_loader, val_loader)\n\u001B[0;32m    121\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtest(model, test_loader)\n",
      "File \u001B[1;32mD:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:70\u001B[0m, in \u001B[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mlist\u001B[39m(env_variables\u001B[38;5;241m.\u001B[39mitems()) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(kwargs\u001B[38;5;241m.\u001B[39mitems()))\n\u001B[0;32m     69\u001B[0m \u001B[38;5;66;03m# all args were already moved to kwargs\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mTypeError\u001B[0m: Trainer.__init__() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
