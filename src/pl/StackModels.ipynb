{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirb28/nn_catalyst/blob/main/src/pl/StackModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning torchmetrics"
      ],
      "metadata": {
        "id": "WA0A-NPNfRW2",
        "outputId": "6012b669-ea10-4314-ed3a-2e7e94316a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch_lightning) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.11.9 pytorch_lightning-2.5.0.post0 torchmetrics-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T02:22:35.690081Z",
          "start_time": "2024-12-07T02:22:31.275132Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olR6CL8he2P8",
        "outputId": "38dcd830-caf8-496c-e98c-252d89f7897a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "import sys, os\n",
        "import pytorch_lightning as pl\n",
        "import torch, math, os\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Colab!\")\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    !pip install pytorch_lightning\n",
        "    !pip install torchmetrics\n",
        "else:\n",
        "    print(\"Not running in Colab.\")\n",
        "\n",
        "def resolve_path_gdrive(relativePath):\n",
        "    if os.path.exists('/content/drive'):\n",
        "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
        "    else:\n",
        "        from utils import get_project_root\n",
        "        return get_project_root() + \"/\" + relativePath\n",
        "\n",
        "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "CHECKPOINTS_FOLDER_BASE = \"/checkpoints/stn_r3_f849_tlast29\"\n",
        "CHECKPOINTS_FOLDER = resolve_path_gdrive(CHECKPOINTS_FOLDER_BASE) #f'd:/temp{CHECKPOINTS_FOLDER_BASE}'\n",
        "DEBUG = False\n",
        "seed = 42\n",
        "pl.seed_everything(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T02:22:36.658730Z",
          "start_time": "2024-12-07T02:22:35.690081Z"
        },
        "id": "1HY0t6qke2P-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
        "from sklearn.metrics import r2_score\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "class MultiTargetDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T02:22:53.320185Z",
          "start_time": "2024-12-07T02:22:53.288534Z"
        },
        "id": "vQzxQ3qae2P_"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "import torchmetrics\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaseModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.r2 = torchmetrics.R2Score()\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log_dict(\n",
        "            {\n",
        "                \"train_loss\": loss,\n",
        "            },\n",
        "            on_step=False,\n",
        "            on_epoch=True,\n",
        "            prog_bar=True,\n",
        "        )\n",
        "        accuracy = self.r2(scores, y)\n",
        "        self.log(\"train_acc\", accuracy, prog_bar=True)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.validation_step_outputs.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
        "        self.log(\"validation_epoch_average\", epoch_average)\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def _common_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        scores = self.forward(x)\n",
        "        loss = self.loss_fn(scores, y)\n",
        "        if DEBUG == True:\n",
        "            print(f\"loss: {loss}, len: {len(y)}\")\n",
        "        return loss, scores, y\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        scores = self.forward(x)\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        return preds\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0.000000001, threshold=0.001)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
        "\n",
        "class SingleTargetNet(BaseModel):\n",
        "\n",
        "    def __init__(self, input_size, learning_rate: float=0.001, dropout_rate: float = 0.2, target=1):\n",
        "        super(SingleTargetNet, self).__init__()\n",
        "        self.lr = learning_rate\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, 1)\n",
        "        self.fc_skip = nn.Linear(1024, 512)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
        "        x1 = self.dropout(x1)\n",
        "\n",
        "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
        "        x2 = self.dropout(x2)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 += self.fc_skip(x1)\n",
        "\n",
        "        x3 = self.fc3(x2)\n",
        "        return x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T02:22:54.337119Z",
          "start_time": "2024-12-07T02:22:54.321095Z"
        },
        "id": "L0aJuhaTe2P_"
      },
      "outputs": [],
      "source": [
        "def train_model(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
        "                y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
        "                input_dim: int, target_num, batch_size: int = 32) -> pl.LightningModule:\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = MultiTargetDataset(X_train, y_train)\n",
        "    val_dataset = MultiTargetDataset(X_val, y_val)\n",
        "    test_dataset = MultiTargetDataset(X_test, y_test)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model with current input dimension\n",
        "    #model = SequentialRegressor(input_dim=input_dim)\n",
        "    model = SingleTargetNet (\n",
        "        input_size=input_dim\n",
        "    )\n",
        "    # Set up callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='train_loss', patience=10, mode='min', verbose=True), #monitor=\"train_loss\", val_loss\n",
        "        ModelCheckpoint(\n",
        "            dirpath=f'{CHECKPOINTS_FOLDER}/{target_num}',\n",
        "            filename='{epoch:02d}-{val_loss:.2f}',\n",
        "            save_top_k=1,\n",
        "            verbose=True,\n",
        "            monitor='val_loss',\n",
        "            mode='min'\n",
        "        ),\n",
        "        LearningRateMonitor(logging_interval='epoch')\n",
        "    ]\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=150,\n",
        "        callbacks=callbacks,\n",
        "        accelerator='auto',\n",
        "        devices=1,\n",
        "        logger=True,\n",
        "        log_every_n_steps=10\n",
        "    )\n",
        "\n",
        "    # Train and test the model\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.test(model, test_loader)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T02:22:55.205337Z",
          "start_time": "2024-12-07T02:22:55.174185Z"
        },
        "id": "oXZh-ws_e2QA"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "def sequential_training(df: pd.DataFrame, num_features: int = 1479,\n",
        "                       num_targets: int = 29, stack_predictions = True, scale_y = True,\n",
        "                       target_range: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n",
        "    X = df.iloc[:, :num_features].values\n",
        "    y = df.iloc[:, num_features:num_features+num_targets].values\n",
        "    global CHECKPOINTS_FOLDER\n",
        "    CHECKPOINTS_FOLDER = f\"{CHECKPOINTS_FOLDER}/stack={stack_predictions}-scaleY={scale_y}\"\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=True)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, shuffle=True)\n",
        "\n",
        "    print(f\"Sizes: {X_train.shape}, {X_val.shape}, {X_test.shape}, {y_train.shape}, {y_val.shape}\")\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Save the scaler parameters\n",
        "    joblib.dump(scaler, f\"{CHECKPOINTS_FOLDER}/scaler_X.pkl\")\n",
        "\n",
        "    if scale_y:\n",
        "        # Create separate scalers for each target\n",
        "        y_scalers = [StandardScaler() for _ in range(num_targets)]\n",
        "        y_train_scaled = np.zeros_like(y_train)\n",
        "        y_val_scaled = np.zeros_like(y_val)\n",
        "        y_test_scaled = np.zeros_like(y_test)\n",
        "\n",
        "        # Scale each target separately\n",
        "        for i in range(num_targets):\n",
        "            y_train_scaled[:, i] = y_scalers[i].fit_transform(y_train[:, i].reshape(-1, 1)).ravel()\n",
        "            y_val_scaled[:, i] = y_scalers[i].transform(y_val[:, i].reshape(-1, 1)).ravel()\n",
        "            y_test_scaled[:, i] = y_scalers[i].transform(y_test[:, i].reshape(-1, 1)).ravel()\n",
        "\n",
        "    final_predictions = pd.DataFrame()\n",
        "\n",
        "    # Initialize current features for each dataset\n",
        "    current_train_features = X_train_scaled.copy()\n",
        "    current_val_features = X_val_scaled.copy()\n",
        "    current_test_features = X_test_scaled.copy()\n",
        "\n",
        "    r2_scores = []\n",
        "\n",
        "    if target_range:\n",
        "        start, end = target_range\n",
        "    else:\n",
        "        start, end = 0, num_targets\n",
        "\n",
        "    for target_idx in range(start, end):\n",
        "        print(f\"\\nTraining model for target {target_idx + 1}/{end - start}. Features: {current_train_features.shape[1]}\")\n",
        "        # Get current target\n",
        "        if scale_y:\n",
        "            current_target = y_train_scaled[:, target_idx].reshape(-1, 1)\n",
        "            current_val_target = y_val_scaled[:, target_idx].reshape(-1, 1)\n",
        "            current_test_target = y_test_scaled[:, target_idx].reshape(-1, 1)\n",
        "        else:\n",
        "            current_target = y_train[:, target_idx].reshape(-1, 1)\n",
        "            current_val_target = y_val[:, target_idx].reshape(-1, 1)\n",
        "            current_test_target = y_test[:, target_idx].reshape(-1, 1)\n",
        "\n",
        "        # Create model with current input dimension\n",
        "        current_input_dim = current_train_features.shape[1]\n",
        "\n",
        "        model = train_model(\n",
        "            current_train_features,\n",
        "            current_val_features,\n",
        "            current_test_features,\n",
        "            current_target,\n",
        "            current_val_target,\n",
        "            current_test_target,\n",
        "            input_dim=current_input_dim,\n",
        "            target_num=target_idx + 1\n",
        "        )\n",
        "        # Make predictions\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Make predictions for training set\n",
        "            train_predictions = model(torch.tensor(current_train_features, dtype=torch.float32))\n",
        "\n",
        "            # Make predictions for validation and test sets\n",
        "            val_predictions = model(torch.tensor(current_val_features, dtype=torch.float32))\n",
        "            test_predictions = model(torch.tensor(current_test_features, dtype=torch.float32))\n",
        "\n",
        "            all_predictions = torch.cat((train_predictions, val_predictions, test_predictions)).cpu().detach().numpy()\n",
        "            all_targets = np.concatenate((current_target, current_val_target, current_test_target))\n",
        "            # Store predictions for current target\n",
        "            # final_predictions[f'train_target_{target_idx+1}_pred'] = train_predictions.flatten()\n",
        "            # final_predictions[f'train_target_{target_idx+1}'] = current_target\n",
        "            #\n",
        "            # final_predictions[f'val_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
        "            # final_predictions[f'val_target_{target_idx+1}'] = current_val_target\n",
        "            #\n",
        "            # final_predictions[f'test_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
        "            # final_predictions[f'test_target_{target_idx+1}'] = current_test_target\n",
        "\n",
        "            final_predictions[f'all_target_{target_idx+1}_pred'] = all_predictions.flatten()\n",
        "            final_predictions[f'all_target_{target_idx+1}'] = all_targets\n",
        "\n",
        "            if stack_predictions == True:\n",
        "                # Update features for next iteration\n",
        "                current_train_features = np.hstack([\n",
        "                    current_train_features,\n",
        "                    train_predictions.cpu().detach().numpy()\n",
        "                ])\n",
        "                current_val_features = np.hstack([\n",
        "                    current_val_features,\n",
        "                    val_predictions.cpu().detach().numpy()\n",
        "                ])\n",
        "                current_test_features = np.hstack([\n",
        "                    current_test_features,\n",
        "                    test_predictions.cpu().detach().numpy()\n",
        "                ])\n",
        "\n",
        "        # Calculate and store R2 score\n",
        "        if scale_y:\n",
        "            test_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
        "                test_predictions.cpu().detach().numpy()\n",
        "            )\n",
        "            r2 = r2_score(y_test[:, target_idx], test_predictions_orig.flatten())\n",
        "        else:\n",
        "            r2 = r2_score(y_test[:, target_idx], test_predictions.flatten())\n",
        "\n",
        "        r2_scores.append(r2)\n",
        "        print(f\"R2 score for target {target_idx + 1}: {r2:.4f}\")\n",
        "\n",
        "    print(\"\\nFinal R2 scores for all targets:\")\n",
        "    for i, r2 in enumerate(r2_scores, start=1):\n",
        "        print(f\"Target {i}: {r2:.4f}\")\n",
        "\n",
        "    return final_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T02:22:59.069534Z",
          "start_time": "2024-12-07T02:22:56.385460Z"
        },
        "id": "UElIIWlQe2QB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "datafile='src/pl/merged_data_f849_tlast29_reordered_byR2.csv'\n",
        "max_rows=None\n",
        "df = pd.read_csv(resolve_path_gdrive(datafile), delimiter=',', skiprows=0, dtype=float, nrows=max_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T05:50:16.332889Z",
          "start_time": "2024-12-07T02:22:59.069829Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI6m6flye2QB",
        "outputId": "70888fb2-37d0-46f7-af21-3bc88ba95edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sizes: (15739, 849), (5247, 849), (5247, 849), (15739, 29), (5247, 29)\n"
          ]
        }
      ],
      "source": [
        "# Run sequential training target_range=[4,30]\n",
        "predictions = sequential_training(df, stack_predictions=False, scale_y=True, target_range=None, num_features=849, num_targets=29)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T19:57:09.118802Z",
          "start_time": "2024-12-07T19:57:07.633046Z"
        },
        "id": "HD7lYt3de2QC"
      },
      "outputs": [],
      "source": [
        "# Save predictions\n",
        "predictions.to_csv(f'{CHECKPOINTS_FOLDER}/predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-07T20:44:17.148915Z",
          "start_time": "2024-12-07T20:44:16.842054Z"
        },
        "id": "X1ne53y1e2QC",
        "outputId": "c1c40ff9-82fa-4125-b942-4e36e1088b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Target stack=F              Name  R2 Train stack=F  R2 Val stack=F  \\\n",
            "0        target_1         elec_en_r          0.993476        0.993806   \n",
            "1        target_2         elec_en_n          0.994522        0.994226   \n",
            "2        target_3           gibbs_r          0.990869        0.993314   \n",
            "3        target_4         elec_en_o          0.993162        0.992971   \n",
            "4        target_5           gibbs_n          0.991933        0.992098   \n",
            "5        target_6           gibbs_o          0.992079        0.993965   \n",
            "6        target_7  homo_spin_down_o          0.987476        0.964452   \n",
            "7        target_8    homo_spin_up_o          0.983732        0.962605   \n",
            "8        target_9            ddg_ox          0.986125        0.956170   \n",
            "9       target_10            lumo_n          0.986401        0.946999   \n",
            "10      target_11    lumo_spin_up_o          0.981682        0.932792   \n",
            "11      target_12  max_charge_pos_n          0.980615        0.936576   \n",
            "12      target_13  max_charge_pos_o          0.976803        0.933966   \n",
            "13      target_14           ddg_red          0.978981        0.925952   \n",
            "14      target_15  lumo_spin_down_o          0.978095        0.940356   \n",
            "15      target_16            homo_n          0.974686        0.915052   \n",
            "16      target_17    lumo_spin_up_r          0.976162        0.913333   \n",
            "17      target_18  max_charge_pos_r          0.946478        0.892119   \n",
            "18      target_19  lumo_spin_down_r          0.952701        0.842810   \n",
            "19      target_20  max_charge_neg_n          0.951245        0.854092   \n",
            "20      target_21  max_charge_neg_o          0.948615        0.843985   \n",
            "21      target_22    homo_spin_up_r          0.949613        0.818680   \n",
            "22      target_23  homo_spin_down_r          0.956968        0.792370   \n",
            "23      target_24  max_charge_neg_r          0.938569        0.776024   \n",
            "24      target_25        max_spin_r          0.873550        0.665244   \n",
            "25      target_26        max_spin_o          0.899606        0.690209   \n",
            "26      target_27          dipole_r          0.823750        0.551665   \n",
            "27      target_28          dipole_o          0.838396        0.568641   \n",
            "28      target_29          dipole_n          0.780781        0.449284   \n",
            "\n",
            "    R2 Test stack=F  R2 All stack=F  \n",
            "0          0.994786        0.993803  \n",
            "1          0.995117        0.994585  \n",
            "2          0.992566        0.991682  \n",
            "3          0.994613        0.993417  \n",
            "4          0.992790        0.992137  \n",
            "5          0.993827        0.992795  \n",
            "6          0.963323        0.978209  \n",
            "7          0.960619        0.974955  \n",
            "8          0.958120        0.974638  \n",
            "9          0.949060        0.971010  \n",
            "10         0.939174        0.963171  \n",
            "11         0.932437        0.962004  \n",
            "12         0.930698        0.959004  \n",
            "13         0.930301        0.958519  \n",
            "14         0.935982        0.962208  \n",
            "15         0.916580        0.951362  \n",
            "16         0.915315        0.951544  \n",
            "17         0.877983        0.921806  \n",
            "18         0.856684        0.911527  \n",
            "19         0.852600        0.912167  \n",
            "20         0.840010        0.906266  \n",
            "21         0.835223        0.900462  \n",
            "22         0.820554        0.896427  \n",
            "23         0.757513        0.869491  \n",
            "24         0.684883        0.794238  \n",
            "25         0.694525        0.815232  \n",
            "26         0.592530        0.718996  \n",
            "27         0.593159        0.735649  \n",
            "28         0.444755        0.647595  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def computeR2(predictions_file, sizes=[15739, 5247, 5247], identifier=''):\n",
        "    # Load the file containing target and predicted values\n",
        "    data = pd.read_csv(resolve_path_gdrive(predictions_file))\n",
        "\n",
        "    # Adjusting the code to match the name pattern 'target_1_pred', 'target_1'\n",
        "    r2_scores = []\n",
        "    [train, test, val] = sizes\n",
        "    for i in range(1, (len(data.columns) // 2) + 1):\n",
        "        train_targets = data[f'all_target_{i}'][:train]\n",
        "        train_predictions = data[f'all_target_{i}_pred'][:train]\n",
        "        val_targets = data[f'all_target_{i}'][train:train+val]\n",
        "        val_predictions = data[f'all_target_{i}_pred'][train:train+val]\n",
        "        test_targets = data[f'all_target_{i}'][train+val:]\n",
        "        test_predictions = data[f'all_target_{i}_pred'][train+val:]\n",
        "\n",
        "        r2 = [r2_score(train_targets, train_predictions), r2_score(val_targets, val_predictions), r2_score(test_targets, test_predictions), r2_score(data[f'all_target_{i}'], data[f'all_target_{i}_pred'])]\n",
        "        r2_scores.append(r2)\n",
        "\n",
        "    # Create a DataFrame to tabulate the results\n",
        "    results = pd.DataFrame({\n",
        "        f'Target {identifier}': [f'target_{i}' for i in range(1, (len(data.columns) // 2) + 1)],\n",
        "        f'Name': df.columns[-29:],\n",
        "        f'R2 Train {identifier}': [score[0] for score in r2_scores],\n",
        "        f'R2 Val {identifier}': [score[1] for score in r2_scores],\n",
        "        f'R2 Test {identifier}': [score[2] for score in r2_scores],\n",
        "        f'R2 All {identifier}': [score[3] for score in r2_scores]\n",
        "    })\n",
        "    return results\n",
        "\n",
        "\n",
        "results_stack_false = computeR2('/checkpoints/stn_r3_f849_tlast29/stack=False-scaleY=True/predictions.csv', identifier='stack=F')\n",
        "#results_stack_true = computeR2('/checkpoints/stn_r3_f849_tlast29/stack=True-scaleY=True/predictions.csv', identifier='stack=T')\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "#final_results = pd.concat([results_stack_false.set_index('Target stack=F'), results_stack_true.set_index('Target stack=T')], axis=1)\n",
        "final_results = results_stack_false\n",
        "print(final_results)\n",
        "final_results.to_csv('final_results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbQ3zqH1e2QD"
      },
      "outputs": [],
      "source": [
        "class SequentialRegressor(pl.LightningModule):\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128],\n",
        "                 dropout_rate: float = 0.2, learning_rate: float = 0.001):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.test_step_outputs = []\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "\n",
        "    def _compute_loss(self, batch: Tuple[torch.Tensor, torch.Tensor],\n",
        "                      stage: str) -> torch.Tensor:\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.MSELoss()(y_hat, y)\n",
        "        self.log(f'{stage}_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor],\n",
        "                     batch_idx: int) -> torch.Tensor:\n",
        "        return self._compute_loss(batch, 'train')\n",
        "\n",
        "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor],\n",
        "                       batch_idx: int) -> torch.Tensor:\n",
        "        return self._compute_loss(batch, 'val')\n",
        "\n",
        "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor],\n",
        "                  batch_idx: int) -> None:\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.MSELoss()(y_hat, y)\n",
        "        self.log('test_loss', loss)\n",
        "        # Detach tensors before storing\n",
        "        self.test_step_outputs.append({\n",
        "            'y_true': y.cpu().detach(),\n",
        "            'y_pred': y_hat.cpu().detach()\n",
        "        })\n",
        "\n",
        "    def on_test_epoch_end(self) -> None:\n",
        "        y_true = torch.cat([out['y_true'] for out in self.test_step_outputs])\n",
        "        y_pred = torch.cat([out['y_pred'] for out in self.test_step_outputs])\n",
        "        # Detach tensors before converting to numpy\n",
        "        r2 = r2_score(y_true.detach().numpy(), y_pred.detach().numpy())\n",
        "        self.log('test_r2', r2, prog_bar=True)\n",
        "        self.test_step_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self) -> Dict:\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': scheduler,\n",
        "            'monitor': 'val_loss'\n",
        "        }\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nn_catalyst",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
