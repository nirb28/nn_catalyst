{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T23:24:00.586821Z",
     "start_time": "2024-12-05T23:24:00.565195Z"
    }
   },
   "source": [
    "%%capture\n",
    "import sys, os\n",
    "import pytorch_lightning as pl\n",
    "import torch, math, os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Colab!\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install torchmetrics\n",
    "else:\n",
    "    print(\"Not running in Colab.\")\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "CHECKPOINTS_FOLDER_BASE = \"/checkpoints/stn_r3_f849_tlast29\"\n",
    "CHECKPOINTS_FOLDER = CHECKPOINTS_FOLDER_BASE\n",
    "DEBUG = False\n",
    "seed = 42\n",
    "\n",
    "pl.seed_everything(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T23:24:02.099152Z",
     "start_time": "2024-12-05T23:24:02.083468Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from sklearn.metrics import r2_score\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T23:24:04.070503Z",
     "start_time": "2024-12-05T23:24:04.046976Z"
    }
   },
   "source": [
    "from torch import nn, optim\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r2 = torchmetrics.R2Score()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        accuracy = self.r2(scores, y)\n",
    "        self.log(\"train_acc\", accuracy, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        if DEBUG == True:\n",
    "            print(f\"loss: {loss}, len: {len(y)}\")\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0.000000001, threshold=0.001)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "class SingleTargetNet(BaseModel):\n",
    "\n",
    "    def __init__(self, input_size, learning_rate: float=0.001, dropout_rate: float = 0.2, target=1):\n",
    "        super(SingleTargetNet, self).__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc_skip = nn.Linear(1024, 512)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 += self.fc_skip(x1)\n",
    "\n",
    "        x3 = self.fc3(x2)\n",
    "        return x3"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T23:24:05.482719Z",
     "start_time": "2024-12-05T23:24:05.467082Z"
    }
   },
   "source": [
    "def train_model(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
    "                y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
    "                input_dim: int, target_num, batch_size: int = 32) -> pl.LightningModule:\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MultiTargetDataset(X_train, y_train)\n",
    "    val_dataset = MultiTargetDataset(X_val, y_val)\n",
    "    test_dataset = MultiTargetDataset(X_test, y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model with current input dimension\n",
    "    #model = SequentialRegressor(input_dim=input_dim)\n",
    "    model = SingleTargetNet (\n",
    "        input_size=input_dim\n",
    "    )    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='train_loss', patience=10, mode='min', verbose=True), #monitor=\"train_loss\", val_loss\n",
    "        ModelCheckpoint(\n",
    "            dirpath=resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/{target_num}'),\n",
    "            filename='{epoch:02d}-{val_loss:.2f}',\n",
    "            save_top_k=2,\n",
    "            verbose=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min'\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ]\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=150,\n",
    "        callbacks=callbacks,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=True,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train and test the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T23:24:06.464092Z",
     "start_time": "2024-12-05T23:24:06.432381Z"
    }
   },
   "source": [
    "def sequential_training(df: pd.DataFrame, num_features: int = 1479, \n",
    "                       num_targets: int = 29, stack_predictions = True, scale_y = True,\n",
    "                       target_range: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n",
    "    X = df.iloc[:, :num_features].values\n",
    "    y = df.iloc[:, num_features:num_features+num_targets].values\n",
    "    global CHECKPOINTS_FOLDER \n",
    "    CHECKPOINTS_FOLDER = f\"{CHECKPOINTS_FOLDER_BASE}/stack={stack_predictions}-scaleY={scale_y}\"\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, shuffle=True)\n",
    "    \n",
    "    print(f\"Sizes: {X_train.shape}, {X_val.shape}, {X_test.shape}, {y_train.shape}, {y_val.shape}\")\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    if scale_y:\n",
    "        # Create separate scalers for each target\n",
    "        y_scalers = [StandardScaler() for _ in range(num_targets)]\n",
    "        y_train_scaled = np.zeros_like(y_train)\n",
    "        y_val_scaled = np.zeros_like(y_val)\n",
    "        y_test_scaled = np.zeros_like(y_test)\n",
    "        \n",
    "        # Scale each target separately\n",
    "        for i in range(num_targets):\n",
    "            y_train_scaled[:, i] = y_scalers[i].fit_transform(y_train[:, i].reshape(-1, 1)).ravel()\n",
    "            y_val_scaled[:, i] = y_scalers[i].transform(y_val[:, i].reshape(-1, 1)).ravel()\n",
    "            y_test_scaled[:, i] = y_scalers[i].transform(y_test[:, i].reshape(-1, 1)).ravel()\n",
    "    \n",
    "    final_predictions = pd.DataFrame()\n",
    "    \n",
    "    # Initialize current features for each dataset\n",
    "    current_train_features = X_train_scaled.copy()\n",
    "    current_val_features = X_val_scaled.copy()\n",
    "    current_test_features = X_test_scaled.copy()\n",
    "    \n",
    "    r2_scores = []\n",
    "    \n",
    "    if target_range:\n",
    "        start, end = target_range\n",
    "    else:\n",
    "        start, end = 0, num_targets\n",
    "    \n",
    "    for target_idx in range(start, end):\n",
    "        print(f\"\\nTraining model for target {target_idx + 1}/{end - start}. Features: {current_train_features.shape[1]}\")\n",
    "        # Get current target        \n",
    "        if scale_y:\n",
    "            current_target = y_train_scaled[:, target_idx].reshape(-1, 1)\n",
    "            current_val_target = y_val_scaled[:, target_idx].reshape(-1, 1)\n",
    "            current_test_target = y_test_scaled[:, target_idx].reshape(-1, 1)\n",
    "        else:\n",
    "            current_target = y_train[:, target_idx].reshape(-1, 1)\n",
    "            current_val_target = y_val[:, target_idx].reshape(-1, 1)\n",
    "            current_test_target = y_test[:, target_idx].reshape(-1, 1)\n",
    "                \n",
    "        # Create model with current input dimension\n",
    "        current_input_dim = current_train_features.shape[1]\n",
    "        \n",
    "        model = train_model(\n",
    "            current_train_features,\n",
    "            current_val_features,\n",
    "            current_test_features,\n",
    "            current_target,\n",
    "            current_val_target,\n",
    "            current_test_target,\n",
    "            input_dim=current_input_dim,\n",
    "            target_num=target_idx + 1\n",
    "        )\n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Make predictions for training set\n",
    "            train_predictions = model(torch.tensor(current_train_features, dtype=torch.float32))\n",
    "            \n",
    "            # Make predictions for validation and test sets\n",
    "            val_predictions = model(torch.tensor(current_val_features, dtype=torch.float32))\n",
    "            test_predictions = model(torch.tensor(current_test_features, dtype=torch.float32))\n",
    "            \n",
    "            all_predictions = torch.cat((train_predictions, val_predictions, test_predictions)).cpu().detach().numpy()\n",
    "            all_targets = np.concatenate((current_target, current_val_target, current_test_target))\n",
    "            # Store predictions for current target\n",
    "            # final_predictions[f'train_target_{target_idx+1}_pred'] = train_predictions.flatten()\n",
    "            # final_predictions[f'train_target_{target_idx+1}'] = current_target\n",
    "            # \n",
    "            # final_predictions[f'val_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
    "            # final_predictions[f'val_target_{target_idx+1}'] = current_val_target\n",
    "            # \n",
    "            # final_predictions[f'test_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
    "            # final_predictions[f'test_target_{target_idx+1}'] = current_test_target\n",
    "            \n",
    "            final_predictions[f'all_target_{target_idx+1}_pred'] = all_predictions.flatten()\n",
    "            final_predictions[f'all_target_{target_idx+1}'] = all_targets\n",
    "\n",
    "            if stack_predictions == True:\n",
    "                # Update features for next iteration\n",
    "                current_train_features = np.hstack([\n",
    "                    current_train_features, \n",
    "                    train_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "                current_val_features = np.hstack([\n",
    "                    current_val_features, \n",
    "                    val_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "                current_test_features = np.hstack([\n",
    "                    current_test_features, \n",
    "                    test_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "        \n",
    "        # Calculate and store R2 score\n",
    "        if scale_y:\n",
    "            test_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
    "                test_predictions.cpu().detach().numpy()\n",
    "            )            \n",
    "            r2 = r2_score(y_test[:, target_idx], test_predictions_orig.flatten())\n",
    "        else:\n",
    "            r2 = r2_score(y_test[:, target_idx], test_predictions.flatten())\n",
    "            \n",
    "        r2_scores.append(r2)\n",
    "        print(f\"R2 score for target {target_idx + 1}: {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal R2 scores for all targets:\")\n",
    "    for i, r2 in enumerate(r2_scores, start=1):\n",
    "        print(f\"Target {i}: {r2:.4f}\")\n",
    "    \n",
    "    return final_predictions\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T23:24:10.471149Z",
     "start_time": "2024-12-05T23:24:07.908954Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "datafile='src/pl/merged_data_f849_tlast29_reordered_byR2.csv'\n",
    "max_rows=None\n",
    "df = pd.read_csv(resolve_path_gdrive(datafile), delimiter=',', skiprows=0, dtype=float, nrows=max_rows)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-05T23:24:57.332743Z"
    }
   },
   "source": [
    "# Run sequential training target_range=[4,30]\n",
    "predictions = sequential_training(df, stack_predictions=False, scale_y=True, target_range=None, num_features=849, num_targets=29)\n",
    "# Save predictions\n",
    "predictions.to_csv(resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/predictions.csv'), index=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: (15739, 849), (5247, 849), (5247, 849), (15739, 29), (5247, 29)\n",
      "\n",
      "Training model for target 1/29. Features: 849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98b72d0899394b56957453f6a3d74266"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ds\\work\\utilities\\conda\\envs\\nn_310_2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "d:\\ds\\work\\utilities\\conda\\envs\\nn_310_2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1aa23f3a62544ef0920eca59d179f9d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cf41f715308412888e45924f414d2fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3dd073c39deb41cf996e148405595dc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "896f3c9327194cf6b8897d37d96ad5c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc1fe14b8162491f83f95ef616d02d7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2675186bb05492fb1e544caccc5b8cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "041f7d539acd4b43b9b930d1355e27c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "953b7c4944554e76be0c1719ea3e107b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f968ec41bf514c918b543bdd09561c07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce637e4eb44e4578af5c826959132de7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5a428a5917a4204beaf245c4bb63fa7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8481c8fc7ac84420a6cfe94698112d76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3284f5a91c843e984082cdb43002962"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5999fe3bf39b4a8e956350adfd4d7cb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad00749f57b74a24a6907acef983922c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save predictions\n",
    "predictions.to_csv(resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/predictions.csv'), index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def computeR2(predictions_file, sizes=[15739, 5247, 5247], identifier=''):\n",
    "    # Load the file containing target and predicted values\n",
    "    data = pd.read_csv(resolve_path_gdrive(predictions_file))\n",
    "    \n",
    "    # Adjusting the code to match the name pattern 'target_1_pred', 'target_1'\n",
    "    r2_scores = []\n",
    "    [train, test, val] = sizes\n",
    "    for i in range(1, (len(data.columns) // 2) + 1):\n",
    "        train_targets = data[f'all_target_{i}'][:train]\n",
    "        train_predictions = data[f'all_target_{i}_pred'][:train]\n",
    "        val_targets = data[f'all_target_{i}'][train:]\n",
    "        val_predictions = data[f'all_target_{i}_pred'][train:]\n",
    "        test_targets = data[f'all_target_{i}'][train+test:]\n",
    "        test_predictions = data[f'all_target_{i}_pred'][train+test:]\n",
    "        \n",
    "        r2 = [r2_score(train_targets, train_predictions), r2_score(val_targets, val_predictions), r2_score(test_targets, test_predictions), r2_score(data[f'all_target_{i}'], data[f'all_target_{i}_pred'])]\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    # Create a DataFrame to tabulate the results\n",
    "    results = pd.DataFrame({\n",
    "        f'Target {identifier}': [f'target_{i}' for i in range(1, (len(data.columns) // 2) + 1)],\n",
    "        f'Name': df.columns[1479:],\n",
    "        f'R2 Train {identifier}': [score[0] for score in r2_scores],\n",
    "        f'R2 Val {identifier}': [score[1] for score in r2_scores],\n",
    "        f'R2 Test {identifier}': [score[2] for score in r2_scores],\n",
    "        f'R2 All {identifier}': [score[3] for score in r2_scores]\n",
    "    })\n",
    "    return results\n",
    "\n",
    "\n",
    "results_stack_false = computeR2('/checkpoints/stn_r2/stack=False-scaleY=True/predictions.csv', identifier='stack=F')\n",
    "results_stack_true = computeR2('/checkpoints/stn_r2/stack=True-scaleY=True/predictions.csv', identifier='stack=T')\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "final_results = pd.concat([results_stack_false.set_index('Target stack=F'), results_stack_true.set_index('Target stack=T')], axis=1)\n",
    "print(final_results)\n",
    "final_results.to_csv('final_results.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SequentialRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], \n",
    "                 dropout_rate: float = 0.2, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "    \n",
    "    def _compute_loss(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                      stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                     batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                       batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                  batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        # Detach tensors before storing\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y.cpu().detach(),\n",
    "            'y_pred': y_hat.cpu().detach()\n",
    "        })\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        y_true = torch.cat([out['y_true'] for out in self.test_step_outputs])\n",
    "        y_pred = torch.cat([out['y_pred'] for out in self.test_step_outputs])\n",
    "        # Detach tensors before converting to numpy\n",
    "        r2 = r2_score(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "        self.log('test_r2', r2, prog_bar=True)\n",
    "        self.test_step_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self) -> Dict:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
