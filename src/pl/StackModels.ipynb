{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:16:00.362033Z",
     "start_time": "2024-12-04T23:15:34.810476Z"
    }
   },
   "source": [
    "%%capture\n",
    "import sys, os\n",
    "import pytorch_lightning as pl\n",
    "import torch, math, os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Colab!\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install torchmetrics\n",
    "else:\n",
    "    print(\"Not running in Colab.\")\n",
    "\n",
    "def resolve_path_gdrive(relativePath):\n",
    "    if os.path.exists('/content/drive'):\n",
    "        return '/content/drive/MyDrive/work/gdrive-workspaces/git/nn_catalyst/' + relativePath\n",
    "    else:\n",
    "        from utils import get_project_root\n",
    "        return get_project_root() + \"/\" + relativePath\n",
    "\n",
    "print(f\"Root project folder is at {resolve_path_gdrive('.')}\")\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "CHECKPOINTS_FOLDER_BASE = \"/checkpoints/stn_r3_f849_tlast29\"\n",
    "CHECKPOINTS_FOLDER = CHECKPOINTS_FOLDER_BASE\n",
    "DEBUG = False\n",
    "seed = 42\n",
    "\n",
    "pl.seed_everything(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:16:17.267270Z",
     "start_time": "2024-12-04T23:16:13.961635Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from sklearn.metrics import r2_score\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:16:20.271573Z",
     "start_time": "2024-12-04T23:16:20.241695Z"
    }
   },
   "source": [
    "from torch import nn, optim\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r2 = torchmetrics.R2Score()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        accuracy = self.r2(scores, y)\n",
    "        self.log(\"train_acc\", accuracy, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        if DEBUG == True:\n",
    "            print(f\"loss: {loss}, len: {len(y)}\")\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(lr=self.lr, params=self.parameters())\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0.000000001, threshold=0.001)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "class SingleTargetNet(BaseModel):\n",
    "\n",
    "    def __init__(self, input_size, learning_rate: float=0.001, dropout_rate: float = 0.2, target=1):\n",
    "        super(SingleTargetNet, self).__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        self.fc_skip = nn.Linear(1024, 512)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.fc1(x)))\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        x2 = F.relu(self.bn2(self.fc2(x1)))\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 += self.fc_skip(x1)\n",
    "\n",
    "        x3 = self.fc3(x2)\n",
    "        return x3"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:16:36.467492Z",
     "start_time": "2024-12-04T23:16:36.453395Z"
    }
   },
   "source": [
    "def train_model(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,\n",
    "                y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,\n",
    "                input_dim: int, target_num, batch_size: int = 32) -> pl.LightningModule:\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MultiTargetDataset(X_train, y_train)\n",
    "    val_dataset = MultiTargetDataset(X_val, y_val)\n",
    "    test_dataset = MultiTargetDataset(X_test, y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model with current input dimension\n",
    "    #model = SequentialRegressor(input_dim=input_dim)\n",
    "    model = SingleTargetNet (\n",
    "        input_size=input_dim\n",
    "    )    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='train_loss', patience=10, mode='min', verbose=True), #monitor=\"train_loss\", val_loss\n",
    "        ModelCheckpoint(\n",
    "            dirpath=resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/{target_num}'),\n",
    "            filename='{epoch:02d}-{val_loss:.2f}',\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min'\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ]\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=150,\n",
    "        callbacks=callbacks,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=True,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train and test the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:16:54.417805Z",
     "start_time": "2024-12-04T23:16:54.386579Z"
    }
   },
   "source": [
    "def sequential_training(df: pd.DataFrame, num_features: int = 1479, \n",
    "                       num_targets: int = 29, stack_predictions = True, scale_y = True,\n",
    "                       target_range: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n",
    "    X = df.iloc[:, :num_features].values\n",
    "    y = df.iloc[:, num_features:num_features+num_targets].values\n",
    "    global CHECKPOINTS_FOLDER \n",
    "    CHECKPOINTS_FOLDER = f\"{CHECKPOINTS_FOLDER_BASE}/stack={stack_predictions}-scaleY={scale_y}\"\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed, shuffle=True)\n",
    "    \n",
    "    print(f\"Sizes: {X_train.shape}, {X_val.shape}, {X_test.shape}, {y_train.shape}, {y_val.shape}\")\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    if scale_y:\n",
    "        # Create separate scalers for each target\n",
    "        y_scalers = [StandardScaler() for _ in range(num_targets)]\n",
    "        y_train_scaled = np.zeros_like(y_train)\n",
    "        y_val_scaled = np.zeros_like(y_val)\n",
    "        y_test_scaled = np.zeros_like(y_test)\n",
    "        \n",
    "        # Scale each target separately\n",
    "        for i in range(num_targets):\n",
    "            y_train_scaled[:, i] = y_scalers[i].fit_transform(y_train[:, i].reshape(-1, 1)).ravel()\n",
    "            y_val_scaled[:, i] = y_scalers[i].transform(y_val[:, i].reshape(-1, 1)).ravel()\n",
    "            y_test_scaled[:, i] = y_scalers[i].transform(y_test[:, i].reshape(-1, 1)).ravel()\n",
    "    \n",
    "    final_predictions = pd.DataFrame()\n",
    "    \n",
    "    # Initialize current features for each dataset\n",
    "    current_train_features = X_train_scaled.copy()\n",
    "    current_val_features = X_val_scaled.copy()\n",
    "    current_test_features = X_test_scaled.copy()\n",
    "    \n",
    "    r2_scores = []\n",
    "    \n",
    "    if target_range:\n",
    "        start, end = target_range\n",
    "    else:\n",
    "        start, end = 0, num_targets\n",
    "    \n",
    "    for target_idx in range(start, end):\n",
    "        print(f\"\\nTraining model for target {target_idx + 1}/{end - start}. Features: {current_train_features.shape[1]}\")\n",
    "        # Get current target        \n",
    "        if scale_y:\n",
    "            current_target = y_train_scaled[:, target_idx].reshape(-1, 1)\n",
    "            current_val_target = y_val_scaled[:, target_idx].reshape(-1, 1)\n",
    "            current_test_target = y_test_scaled[:, target_idx].reshape(-1, 1)\n",
    "        else:\n",
    "            current_target = y_train[:, target_idx].reshape(-1, 1)\n",
    "            current_val_target = y_val[:, target_idx].reshape(-1, 1)\n",
    "            current_test_target = y_test[:, target_idx].reshape(-1, 1)\n",
    "                \n",
    "        # Create model with current input dimension\n",
    "        current_input_dim = current_train_features.shape[1]\n",
    "        \n",
    "        model = train_model(\n",
    "            current_train_features,\n",
    "            current_val_features,\n",
    "            current_test_features,\n",
    "            current_target,\n",
    "            current_val_target,\n",
    "            current_test_target,\n",
    "            input_dim=current_input_dim,\n",
    "            target_num=target_idx + 1\n",
    "        )\n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Make predictions for training set\n",
    "            train_predictions = model(torch.tensor(current_train_features, dtype=torch.float32))\n",
    "            \n",
    "            # Make predictions for validation and test sets\n",
    "            val_predictions = model(torch.tensor(current_val_features, dtype=torch.float32))\n",
    "            test_predictions = model(torch.tensor(current_test_features, dtype=torch.float32))\n",
    "            \n",
    "            all_predictions = torch.cat((train_predictions, val_predictions, test_predictions)).cpu().detach().numpy()\n",
    "            all_targets = np.concatenate((current_target, current_val_target, current_test_target))\n",
    "            # Store predictions for current target\n",
    "            # final_predictions[f'train_target_{target_idx+1}_pred'] = train_predictions.flatten()\n",
    "            # final_predictions[f'train_target_{target_idx+1}'] = current_target\n",
    "            # \n",
    "            # final_predictions[f'val_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
    "            # final_predictions[f'val_target_{target_idx+1}'] = current_val_target\n",
    "            # \n",
    "            # final_predictions[f'test_target_{target_idx+1}_pred'] = test_predictions.flatten()\n",
    "            # final_predictions[f'test_target_{target_idx+1}'] = current_test_target\n",
    "            \n",
    "            final_predictions[f'all_target_{target_idx+1}_pred'] = all_predictions.flatten()\n",
    "            final_predictions[f'all_target_{target_idx+1}'] = all_targets\n",
    "\n",
    "            if stack_predictions == True:\n",
    "                # Update features for next iteration\n",
    "                current_train_features = np.hstack([\n",
    "                    current_train_features, \n",
    "                    train_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "                current_val_features = np.hstack([\n",
    "                    current_val_features, \n",
    "                    val_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "                current_test_features = np.hstack([\n",
    "                    current_test_features, \n",
    "                    test_predictions.cpu().detach().numpy()\n",
    "                ])\n",
    "        \n",
    "        # Calculate and store R2 score\n",
    "        if scale_y:\n",
    "            test_predictions_orig = y_scalers[target_idx].inverse_transform(\n",
    "                test_predictions.cpu().detach().numpy()\n",
    "            )            \n",
    "            r2 = r2_score(y_test[:, target_idx], test_predictions_orig.flatten())\n",
    "        else:\n",
    "            r2 = r2_score(y_test[:, target_idx], test_predictions.flatten())\n",
    "            \n",
    "        r2_scores.append(r2)\n",
    "        print(f\"R2 score for target {target_idx + 1}: {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal R2 scores for all targets:\")\n",
    "    for i, r2 in enumerate(r2_scores, start=1):\n",
    "        print(f\"Target {i}: {r2:.4f}\")\n",
    "    \n",
    "    return final_predictions\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:17:20.468731Z",
     "start_time": "2024-12-04T23:17:16.134352Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "datafile='src/pl/merged_data_f849_tlast29_reordered_byR2.csv'\n",
    "max_rows=None\n",
    "df = pd.read_csv(resolve_path_gdrive(datafile), delimiter=',', skiprows=0, dtype=float, nrows=max_rows)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-04T23:25:32.732115Z"
    }
   },
   "source": [
    "# Run sequential training\n",
    "#predictions = sequential_training(df, stack_predictions=False, scale_y=True, target_range=None)\n",
    "# Save predictions\n",
    "#predictions.to_csv(resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/predictions.csv'), index=False)\n",
    "\n",
    "# Run sequential training\n",
    "predictions = sequential_training(df, stack_predictions=True, scale_y=True, target_range=None, \n",
    "                                  num_features=849, num_targets=29)\n",
    "# Save predictions\n",
    "predictions.to_csv(resolve_path_gdrive(f'{CHECKPOINTS_FOLDER}/predictions.csv'), index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: (15739, 849), (5247, 849), (5247, 849), (15739, 29), (5247, 29)\n",
      "\n",
      "Training model for target 1/29. Features: 849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f711f1f003147bcae1cae79ba521f55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "D:\\ds\\work\\utilities\\python\\envs\\llm-dev\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f789f5377644aaaa1250edb09acdee2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48ed67f7a24346799fbf0e2f17406f89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:34:47.940652Z",
     "start_time": "2024-11-19T01:34:47.324170Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def computeR2(predictions_file, sizes=[15739, 5247, 5247], identifier=''):\n",
    "    # Load the file containing target and predicted values\n",
    "    data = pd.read_csv(resolve_path_gdrive(predictions_file))\n",
    "    \n",
    "    # Adjusting the code to match the name pattern 'target_1_pred', 'target_1'\n",
    "    r2_scores = []\n",
    "    [train, test, val] = sizes\n",
    "    for i in range(1, (len(data.columns) // 2) + 1):\n",
    "        train_targets = data[f'all_target_{i}'][:train]\n",
    "        train_predictions = data[f'all_target_{i}_pred'][:train]\n",
    "        val_targets = data[f'all_target_{i}'][train:]\n",
    "        val_predictions = data[f'all_target_{i}_pred'][train:]\n",
    "        test_targets = data[f'all_target_{i}'][train+test:]\n",
    "        test_predictions = data[f'all_target_{i}_pred'][train+test:]\n",
    "        \n",
    "        r2 = [r2_score(train_targets, train_predictions), r2_score(val_targets, val_predictions), r2_score(test_targets, test_predictions), r2_score(data[f'all_target_{i}'], data[f'all_target_{i}_pred'])]\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    # Create a DataFrame to tabulate the results\n",
    "    results = pd.DataFrame({\n",
    "        f'Target {identifier}': [f'target_{i}' for i in range(1, (len(data.columns) // 2) + 1)],\n",
    "        f'Name': df.columns[1479:],\n",
    "        f'R2 Train {identifier}': [score[0] for score in r2_scores],\n",
    "        f'R2 Val {identifier}': [score[1] for score in r2_scores],\n",
    "        f'R2 Test {identifier}': [score[2] for score in r2_scores],\n",
    "        f'R2 All {identifier}': [score[3] for score in r2_scores]\n",
    "    })\n",
    "    return results\n",
    "\n",
    "\n",
    "results_stack_false = computeR2('/checkpoints/stn_r2/stack=False-scaleY=True/predictions.csv', identifier='stack=F')\n",
    "results_stack_true = computeR2('/checkpoints/stn_r2/stack=True-scaleY=True/predictions.csv', identifier='stack=T')\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "final_results = pd.concat([results_stack_false.set_index('Target stack=F'), results_stack_true.set_index('Target stack=T')], axis=1)\n",
    "print(final_results)\n",
    "final_results.to_csv('final_results.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Name  R2 Train stack=F  R2 Val stack=F  \\\n",
      "target_1          elec_en_r          0.996551        0.996081   \n",
      "target_2          elec_en_n          0.993842        0.993672   \n",
      "target_3            gibbs_r          0.992557        0.992553   \n",
      "target_4          elec_en_o          0.993708        0.992917   \n",
      "target_5            gibbs_n          0.992110        0.993177   \n",
      "target_6            gibbs_o          0.994168        0.993899   \n",
      "target_7   homo_spin_down_o          0.987488        0.962685   \n",
      "target_8     homo_spin_up_o          0.986880        0.961077   \n",
      "target_9             ddg_ox          0.988268        0.955527   \n",
      "target_10            lumo_n          0.987634        0.947394   \n",
      "target_11    lumo_spin_up_o          0.987347        0.935026   \n",
      "target_12  max_charge_pos_n          0.982738        0.935436   \n",
      "target_13  max_charge_pos_o          0.981268        0.928376   \n",
      "target_14           ddg_red          0.981008        0.927756   \n",
      "target_15  lumo_spin_down_o          0.985186        0.938749   \n",
      "target_16            homo_n          0.982314        0.914880   \n",
      "target_17    lumo_spin_up_r          0.965241        0.905501   \n",
      "target_18  max_charge_pos_r          0.967951        0.887976   \n",
      "target_19  lumo_spin_down_r          0.971562        0.844311   \n",
      "target_20  max_charge_neg_n          0.953580        0.838292   \n",
      "target_21  max_charge_neg_o          0.953059        0.840927   \n",
      "target_22    homo_spin_up_r          0.963642        0.822625   \n",
      "target_23  homo_spin_down_r          0.954297        0.806840   \n",
      "target_24  max_charge_neg_r          0.948423        0.761639   \n",
      "target_25        max_spin_r          0.903194        0.680382   \n",
      "target_26        max_spin_o          0.924211        0.679324   \n",
      "target_27          dipole_r          0.905751        0.561080   \n",
      "target_28          dipole_o          0.845127        0.581821   \n",
      "target_29          dipole_n          0.813163        0.457794   \n",
      "\n",
      "           R2 Test stack=F  R2 All stack=F              Name  \\\n",
      "target_1          0.996828        0.996366         elec_en_r   \n",
      "target_2          0.993933        0.993775         elec_en_n   \n",
      "target_3          0.993400        0.992555           gibbs_r   \n",
      "target_4          0.993662        0.993396         elec_en_o   \n",
      "target_5          0.993735        0.992530           gibbs_n   \n",
      "target_6          0.994462        0.994062           gibbs_o   \n",
      "target_7          0.963468        0.977737  homo_spin_down_o   \n",
      "target_8          0.961132        0.976635    homo_spin_up_o   \n",
      "target_9          0.958305        0.975294            ddg_ox   \n",
      "target_10         0.948349        0.971494            lumo_n   \n",
      "target_11         0.938823        0.966169    lumo_spin_up_o   \n",
      "target_12         0.936330        0.963634  max_charge_pos_n   \n",
      "target_13         0.930338        0.960094  max_charge_pos_o   \n",
      "target_14         0.928185        0.959581           ddg_red   \n",
      "target_15         0.936117        0.966699  lumo_spin_down_o   \n",
      "target_16         0.916871        0.955603            homo_n   \n",
      "target_17         0.908707        0.941459    lumo_spin_up_r   \n",
      "target_18         0.881227        0.935781  max_charge_pos_r   \n",
      "target_19         0.849353        0.920665  lumo_spin_down_r   \n",
      "target_20         0.853965        0.907538  max_charge_neg_n   \n",
      "target_21         0.838017        0.908488  max_charge_neg_o   \n",
      "target_22         0.827996        0.907099    homo_spin_up_r   \n",
      "target_23         0.819489        0.895018  homo_spin_down_r   \n",
      "target_24         0.753485        0.873064  max_charge_neg_r   \n",
      "target_25         0.689887        0.814044        max_spin_r   \n",
      "target_26         0.680088        0.824535        max_spin_o   \n",
      "target_27         0.580594        0.762574          dipole_r   \n",
      "target_28         0.595345        0.740152          dipole_o   \n",
      "target_29         0.466290        0.671339          dipole_n   \n",
      "\n",
      "           R2 Train stack=T  R2 Val stack=T  R2 Test stack=T  R2 All stack=T  \n",
      "target_1           0.994227        0.994258         0.994854        0.994239  \n",
      "target_2           0.993705        0.994046         0.994403        0.993839  \n",
      "target_3           0.992562        0.993023         0.993385        0.992744  \n",
      "target_4           0.995646        0.995200         0.996163        0.995470  \n",
      "target_5           0.994739        0.995108         0.995503        0.994884  \n",
      "target_6           0.993212        0.994637         0.994620        0.993774  \n",
      "target_7           0.985752        0.962099         0.963961        0.976453  \n",
      "target_8           0.986315        0.961124         0.961382        0.976313  \n",
      "target_9           0.989747        0.956481         0.958168        0.976565  \n",
      "target_10          0.987224        0.946367         0.947875        0.970837  \n",
      "target_11          0.986197        0.930709         0.939322        0.963737  \n",
      "target_12          0.981953        0.936797         0.935684        0.963716  \n",
      "target_13          0.984610        0.934246         0.932762        0.964448  \n",
      "target_14          0.983691        0.929322         0.930609        0.961815  \n",
      "target_15          0.983811        0.939201         0.938149        0.966052  \n",
      "target_16          0.977255        0.914956         0.917228        0.952578  \n",
      "target_17          0.972007        0.908576         0.911546        0.946756  \n",
      "target_18          0.969128        0.893543         0.888395        0.938724  \n",
      "target_19          0.973896        0.848078         0.853145        0.923572  \n",
      "target_20          0.949668        0.835359         0.850358        0.904017  \n",
      "target_21          0.954132        0.837510         0.839209        0.907776  \n",
      "target_22          0.977603        0.816391         0.825322        0.912963  \n",
      "target_23          0.950136        0.813925         0.826325        0.895378  \n",
      "target_24          0.951827        0.766902         0.766311        0.877218  \n",
      "target_25          0.920048        0.679626         0.692600        0.823850  \n",
      "target_26          0.949116        0.688905         0.685975        0.843202  \n",
      "target_27          0.872887        0.584616         0.604335        0.753151  \n",
      "target_28          0.845747        0.565088         0.600843        0.733853  \n",
      "target_29          0.840064        0.448064         0.451985        0.683618  \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], \n",
    "                 dropout_rate: float = 0.2, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "    \n",
    "    def _compute_loss(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                      stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                     batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                       batch_idx: int) -> torch.Tensor:\n",
    "        return self._compute_loss(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n",
    "                  batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        # Detach tensors before storing\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y.cpu().detach(),\n",
    "            'y_pred': y_hat.cpu().detach()\n",
    "        })\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        y_true = torch.cat([out['y_true'] for out in self.test_step_outputs])\n",
    "        y_pred = torch.cat([out['y_pred'] for out in self.test_step_outputs])\n",
    "        # Detach tensors before converting to numpy\n",
    "        r2 = r2_score(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "        self.log('test_r2', r2, prog_bar=True)\n",
    "        self.test_step_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self) -> Dict:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
